"""æœ¬åœ°æ¨¡å‹é—®é¢˜ç”Ÿæˆå™¨å®ç° - æ”¯æŒOllama"""

import re
import uuid
import requests
from typing import Any, Dict, List, Optional, Sequence, TYPE_CHECKING
from datetime import datetime
from loguru import logger

from ..interfaces.question_generator_interface import QuestionGeneratorInterface, QuestionGenerationError
from ..models.document import DocumentChunk
from ..models.question import Question, QuestionSet
from ..utils.config import ConfigManager
from ..utils.lightrag_utils import compute_lightrag_chunk_id, LightRAGContextBuilder

if TYPE_CHECKING:
    from .lightrag_rag import LightRAGImplementation

# å¯¼å…¥è¶…æ—¶é…ç½®
try:
    from ...timeout_config import configure_global_timeouts, configure_ollama_timeout
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    def configure_global_timeouts():
        return requests.Session()
    
    def configure_ollama_timeout():
        return {'timeout': (60, 30000)}


class LocalQuestionGenerator(QuestionGeneratorInterface):
    """åŸºäºOllamaçš„æœ¬åœ°é—®é¢˜ç”Ÿæˆå™¨å®ç°"""

    def __init__(
        self,
        config: ConfigManager,
        rag: Optional["LightRAGImplementation"] = None,
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹é—®é¢˜ç”Ÿæˆå™¨

        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.rag = rag
        
        # Ollamaé…ç½®
        self.model_name = config.get("question_generator.local.model_name", "deepseek-r1:32b")
        self.base_url = config.get("question_generator.local.base_url", "http://localhost:11434")
        self.max_tokens = config.get("question_generator.local.max_tokens", 2048)
        self.temperature = config.get("question_generator.local.temperature", 0.7)
        self.timeout = config.get("question_generator.local.timeout", 120)
        self.questions_per_chunk = config.get("question_generator.local.questions_per_chunk", 10)

        # çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡é…ç½®
        self.enable_kg_context = config.get(
            "question_generator.local.enable_kg_context", True
        )
        self.max_context_entities = config.get(
            "question_generator.local.max_context_entities", 3
        )
        self.max_context_relations = config.get(
            "question_generator.local.max_context_relations", 2
        )
        self.max_context_snippets = config.get(
            "question_generator.local.max_context_snippets", 2
        )
        self.context_snippet_chars = config.get(
            "question_generator.local.context_snippet_chars", 200
        )
        self.max_related_chunk_ids = config.get(
            "question_generator.local.max_related_chunk_ids", 6
        )

        if not rag or not getattr(rag, "rag", None):
            self.enable_kg_context = False

        self.context_builder: Optional[LightRAGContextBuilder] = None
        if self.enable_kg_context:
            self.context_builder = LightRAGContextBuilder(
                rag,
                max_entities=self.max_context_entities,
                max_relations=self.max_context_relations,
                max_snippets=self.max_context_snippets,
                snippet_chars=self.context_snippet_chars,
                max_related_chunk_ids=self.max_related_chunk_ids,
            )

        # åŠ è½½æç¤ºè¯
        self.system_prompt = config.get("prompts.system_prompt", "")
        self.human_prompt = config.get("prompts.human_prompt", "")

        # æµ‹è¯•è¿æ¥
        if not self._test_connection():
            raise QuestionGenerationError(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {self.base_url}")

        # é…ç½®å…¨å±€è¶…æ—¶è®¾ç½®
        self.session = configure_global_timeouts()
        self.ollama_config = configure_ollama_timeout()
        
        logger.info(f"æœ¬åœ°æ¨¡å‹é—®é¢˜ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.model_name}")

    def _test_connection(self) -> bool:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            return response.status_code == 200
        except:
            return False

    def generate_questions_from_chunk(self, chunk: DocumentChunk) -> List[Question]:
        """
        ä»å•ä¸ªæ–‡æœ¬å—ç”Ÿæˆé—®é¢˜

        Args:
            chunk: è¦ç”Ÿæˆé—®é¢˜çš„DocumentChunk

        Returns:
            Questionå¯¹è±¡åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹ä¸ºå—ç”Ÿæˆé—®é¢˜: {chunk.chunk_id}")
            logger.info(f"ğŸ“„ æ–‡æœ¬å—é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
            logger.info(f"ğŸ¯ ç›®æ ‡é—®é¢˜æ•°é‡: {self.questions_per_chunk}")

            context_package = self._build_context_for_chunk(chunk)
            prompt_text = self._compose_prompt_text(chunk.content)
            prompt_context = context_package.get("prompt_context", "")

            # å‡†å¤‡æç¤ºè¯
            human_message = self.human_prompt.format(
                text=prompt_text,
                prompt_context=prompt_context,
                questions_per_chunk=self.questions_per_chunk
            )
            
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(human_message)} å­—ç¬¦")
            logger.info(f"ğŸ¤– è°ƒç”¨æœ¬åœ°æ¨¡å‹: {self.model_name}")

            # è°ƒç”¨Ollama API
            response_content = self._call_ollama_api(human_message)

            logger.info(f"âœ… æ”¶åˆ°æœ¬åœ°æ¨¡å‹å“åº”: {len(response_content)} å­—ç¬¦")
            logger.info(f"ğŸ“‹ åŸå§‹å“åº”é¢„è§ˆ: {response_content[:200]}...")

            # è§£æé—®é¢˜
            questions = self.parse_questions_from_response(
                response_content, chunk, context_package
            )

            logger.info(f"ğŸ‰ æˆåŠŸä¸ºå— {chunk.chunk_id} ç”Ÿæˆäº† {len(questions)} ä¸ªé—®é¢˜")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„é—®é¢˜
            for i, question in enumerate(questions, 1):
                logger.info(f"  é—®é¢˜{i}: {question.content[:100]}{'...' if len(question.content) > 100 else ''}")
            
            return questions

        except Exception as e:
            raise QuestionGenerationError(f"ä¸ºå— {chunk.chunk_id} ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")

    def _extract_candidate_entities(self, text: str) -> List[str]:
        """ä»é—®é¢˜æ–‡æœ¬ä¸­æå–å¯èƒ½çš„å®ä½“åç§°æˆ–å‹å·ã€‚"""
        if not text:
            return []

        candidates: List[str] = []
        patterns = [
            r"[A-Z]{2,}\d+[A-Z]*",
            r"[A-Z]+\d+[A-Z0-9]*",
            r"[A-Z][A-Za-z0-9\-]{2,}",
        ]

        matched_tokens = set()
        for pattern in patterns:
            matched_tokens.update(re.findall(pattern, text))

        # æŒ‰å‡ºç°é¡ºåºå»é‡
        for token in re.findall(r"\b[^\s]+\b", text):
            normalized = token.strip(".,;:!?ï¼Œã€‚ï¼›ï¼šï¼ˆï¼‰()[]{}â€œâ€\"'")
            if normalized in matched_tokens and normalized not in candidates:
                candidates.append(normalized)

        return candidates

    def _empty_context_package(self) -> Dict[str, Any]:
        return {
            "prompt_context": "",
            "related_entities": [],
            "related_chunk_ids": [],
        }

    def _build_context_for_chunk(self, chunk: DocumentChunk) -> Dict[str, Any]:
        if not self.context_builder:
            return self._empty_context_package()

        chunk_id = compute_lightrag_chunk_id(chunk.content)
        if not chunk_id:
            return self._empty_context_package()

        try:
            context = self.context_builder.build_context(chunk_id)
        except Exception as e:
            logger.debug(f"æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡å¤±è´¥ï¼ˆchunk: {chunk.chunk_id}ï¼‰: {e}")
            context = LightRAGContextBuilder._empty_context()

        if not context:
            return self._empty_context_package()

        return {
            "prompt_context": context.get("prompt_context", ""),
            "related_entities": context.get("related_entities", []) or [],
            "related_chunk_ids": context.get("related_chunk_ids", []) or [],
        }

    def _compose_prompt_text(self, chunk_text: str) -> str:
        return (chunk_text or "").strip()

    def _build_question_object(
        self,
        question_content: str,
        source_chunk: DocumentChunk,
        question_index: int,
        base_related_entities: Sequence[str],
        base_related_chunk_ids: Sequence[str],
        primary_chunk_id: Optional[str],
        knowledge_used: bool,
    ) -> Question:
        candidate_entities = self._extract_candidate_entities(question_content)
        combined_entities = list(
            dict.fromkeys(list(base_related_entities) + candidate_entities)
        )

        metadata: Dict[str, Any] = {"has_answer": False}
        if primary_chunk_id:
            metadata["lightrag_chunk_id"] = primary_chunk_id
        if combined_entities:
            metadata["related_entities"] = combined_entities
        if base_related_chunk_ids:
            metadata["related_chunk_ids"] = list(
                dict.fromkeys(base_related_chunk_ids)
            )
        if knowledge_used:
            metadata["knowledge_context_used"] = True

        question = Question(
            question_id=str(uuid.uuid4()),
            content=question_content,
            source_document=source_chunk.document_id,
            source_chunk_id=source_chunk.chunk_id,
            question_index=question_index,
            created_at=datetime.now(),
            metadata=metadata,
            source_chunk_content=source_chunk.content,
            related_entities=combined_entities,
        )
        return question

    def _call_ollama_api(self, prompt: str) -> str:
        """è°ƒç”¨Ollama API"""
        try:
            # è®¾ç½®å…¨å±€requestsè¶…æ—¶
            import os
            import time
            os.environ['REQUESTS_TIMEOUT'] = str(self.timeout)
            
            payload = {
                "model": self.model_name,
                "prompt": f"{self.system_prompt}\n\n{prompt}",
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            }
            
            logger.info(f"ğŸš€ å‘é€è¯·æ±‚åˆ°: {self.base_url}/api/generate")
            logger.info(f"â±ï¸  è¶…æ—¶è®¾ç½®: {self.timeout} ç§’")
            logger.info(f"ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {self.temperature}")
            logger.info(f"ğŸ“Š æœ€å¤§tokenæ•°: {self.max_tokens}")
            
            start_time = time.time()
            
            # ä½¿ç”¨é…ç½®å¥½çš„sessionå’Œè¶…æ—¶è®¾ç½®
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                **self.ollama_config
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"â° APIè°ƒç”¨è€—æ—¶: {duration:.2f} ç§’")
            logger.info(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "")
                logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                return response_text
            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                logger.error(f"ğŸ“„ é”™è¯¯å“åº”: {response.text}")
                raise QuestionGenerationError(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")

        except requests.exceptions.Timeout:
            logger.error(f"â° APIè°ƒç”¨è¶…æ—¶ (è¶…è¿‡ {self.timeout} ç§’)")
            raise QuestionGenerationError("Ollama APIè°ƒç”¨è¶…æ—¶")
        except Exception as e:
            logger.error(f"ğŸ’¥ APIè°ƒç”¨å¼‚å¸¸: {e}")
            raise QuestionGenerationError(f"Ollama APIè°ƒç”¨å¼‚å¸¸: {e}")

    def generate_questions_from_chunks(self, chunks: List[DocumentChunk]) -> QuestionSet:
        """
        ä»å¤šä¸ªæ–‡æœ¬å—ç”Ÿæˆé—®é¢˜

        Args:
            chunks: DocumentChunkå¯¹è±¡åˆ—è¡¨

        Returns:
            åŒ…å«æ‰€æœ‰ç”Ÿæˆé—®é¢˜çš„QuestionSet
        """
        try:
            if not chunks:
                raise QuestionGenerationError("æ²¡æœ‰æä¾›ç”¨äºé—®é¢˜ç”Ÿæˆçš„å—")

            document_id = chunks[0].document_id
            logger.info(f"ä¸ºæ–‡æ¡£ {document_id} çš„ {len(chunks)} ä¸ªå—ç”Ÿæˆé—®é¢˜")

            all_questions = []

            for chunk in chunks:
                try:
                    chunk_questions = self.generate_questions_from_chunk(chunk)
                    all_questions.extend(chunk_questions)
                except Exception as e:
                    logger.error(f"ä¸ºå— {chunk.chunk_id} ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                    continue

            # åˆ›å»ºQuestionSet
            question_set = QuestionSet(
                document_id=document_id,
                questions=all_questions,
                created_at=datetime.now()
            )

            logger.info(f"ä¸ºæ–‡æ¡£ {document_id} æ€»å…±ç”Ÿæˆäº† {len(all_questions)} ä¸ªé—®é¢˜")
            return question_set

        except Exception as e:
            raise QuestionGenerationError(f"ä»å—ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")

    def parse_questions_from_response(
        self,
        response: str,
        source_chunk: DocumentChunk,
        context_package: Dict[str, Any],
    ) -> List[Question]:
        """
        ä»LLMå“åº”ä¸­è§£æé—®é¢˜ï¼ˆåªè§£æé—®é¢˜ï¼Œä¸è§£æç­”æ¡ˆï¼‰
        """
        try:
            cleaned_response = self._clean_think_tags(response)
            questions: List[Question] = []

            base_related_entities = list(
                dict.fromkeys(context_package.get("related_entities", []))
            )
            base_related_chunk_ids = list(
                dict.fromkeys(context_package.get("related_chunk_ids", []))
            )
            primary_chunk_id = compute_lightrag_chunk_id(source_chunk.content)
            if primary_chunk_id and primary_chunk_id not in base_related_chunk_ids:
                base_related_chunk_ids = [primary_chunk_id] + base_related_chunk_ids
            knowledge_used = bool(context_package.get("prompt_context"))

            question_pattern = r"é—®é¢˜(\d+)[:ï¼š]\s*(.+?)(?=\n\s*é—®é¢˜\d+[:ï¼š]|$)"
            question_matches = re.findall(question_pattern, cleaned_response, re.DOTALL)

            if question_matches:
                logger.info(f"âœ… æ‰¾åˆ°æ–°æ ¼å¼é—®é¢˜å€™é€‰: {len(question_matches)} ä¸ª")
                for match in question_matches:
                    question_num = int(match[0])
                    question_content = match[1].strip()
                    question_content = re.sub(r"^é—®é¢˜[:ï¼š]\s*", "", question_content)
                    question_content = re.sub(r"\n+", " ", question_content).strip()

                    is_valid = (
                        question_content
                        and len(question_content) > 15
                        and ("ï¼Ÿ" in question_content or "?" in question_content)
                        and not re.match(r"^#+\s", question_content)
                        and not re.match(r"^(å¤æ‚|ä¸­ç­‰|ç®€å•|å…³è”|æ·±åº¦|äº‹å®).*é—®é¢˜", question_content)
                        and not question_content.startswith("ã€")
                    )

                    if is_valid:
                        question = self._build_question_object(
                            question_content=question_content,
                            source_chunk=source_chunk,
                            question_index=question_num,
                            base_related_entities=base_related_entities,
                            base_related_chunk_ids=base_related_chunk_ids,
                            primary_chunk_id=primary_chunk_id,
                            knowledge_used=knowledge_used,
                        )
                        questions.append(question)
                        logger.debug(f"âœ… æœ‰æ•ˆé—®é¢˜ {question_num}: {question_content[:50]}...")
                    else:
                        logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆå†…å®¹ {question_num}: {question_content[:50]}...")

            if not questions:
                logger.info("âš ï¸ æœªæ‰¾åˆ°æ–°æ ¼å¼é—®é¢˜ï¼Œå°è¯•å…¼å®¹æ—§æ ¼å¼ï¼ˆé—®ç­”å¯¹æ ¼å¼ï¼‰...")
                qa_pair_pattern = r"é—®ç­”å¯¹(\d+)[:ï¼š]\s*\n\s*é—®é¢˜[:ï¼š]\s*(.+?)(?:\s*\n\s*ç­”æ¡ˆ[:ï¼š]|(?=\n\s*é—®ç­”å¯¹\d+|$))"
                qa_matches = re.findall(qa_pair_pattern, cleaned_response, re.DOTALL)

                if qa_matches:
                    logger.info(f"âœ… æ‰¾åˆ°æ—§æ ¼å¼é—®ç­”å¯¹ï¼ˆä»…æå–é—®é¢˜ï¼‰: {len(qa_matches)} ä¸ª")
                    for match in qa_matches:
                        qa_num = int(match[0])
                        question_content = re.sub(r"^é—®é¢˜[:ï¼š]\s*", "", match[1]).strip()

                        if question_content:
                            question = self._build_question_object(
                                question_content=question_content,
                                source_chunk=source_chunk,
                                question_index=qa_num,
                                base_related_entities=base_related_entities,
                                base_related_chunk_ids=base_related_chunk_ids,
                                primary_chunk_id=primary_chunk_id,
                                knowledge_used=knowledge_used,
                            )
                            questions.append(question)
                            logger.debug(f"é—®é¢˜ {qa_num}: {question_content[:50]}...")

            if not questions:
                logger.error("âŒ æ‰€æœ‰æ ¼å¼éƒ½æœªåŒ¹é…ï¼Œå°è¯•fallbackæå–...")
                questions = self._extract_fallback_questions(
                    cleaned_response,
                    source_chunk,
                    context_package,
                    start_index=1,
                )

            logger.info(f"ä»å“åº”ä¸­è§£æå‡º {len(questions)} ä¸ªé—®é¢˜")
            return questions

        except Exception as e:
            raise QuestionGenerationError(f"ä»å“åº”è§£æé—®é¢˜å¤±è´¥: {e}")

    def _clean_think_tags(self, text: str) -> str:
        """
        æ¸…ç†DeepSeek R1çš„<think>æ ‡ç­¾å’Œå†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤<think>æ ‡ç­¾åŠå…¶å†…å®¹
        cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)
        
        return cleaned_text.strip()

    def validate_questions(self, questions: List[Question]) -> bool:
        """
        éªŒè¯ç”Ÿæˆçš„é—®é¢˜

        Args:
            questions: è¦éªŒè¯çš„é—®é¢˜åˆ—è¡¨

        Returns:
            å¦‚æœæ‰€æœ‰é—®é¢˜éƒ½æœ‰æ•ˆåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not questions:
            return False

        for question in questions:
            # æ£€æŸ¥é—®é¢˜æ˜¯å¦æœ‰å†…å®¹
            if not question.content or not question.content.strip():
                logger.error(f"é—®é¢˜ {question.question_id} æ²¡æœ‰å†…å®¹")
                return False

            # æ£€æŸ¥é—®é¢˜æ˜¯å¦å¤ªçŸ­
            if len(question.content.strip()) < 10:
                logger.error(f"é—®é¢˜ {question.question_id} å¤ªçŸ­")
                return False

            # æ£€æŸ¥é—®é¢˜æ ¼å¼
            if not question.content.startswith("é—®é¢˜"):
                logger.warning(f"é—®é¢˜ {question.question_id} ä¸ä»¥'é—®é¢˜'å¼€å¤´")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not question.source_document or not question.source_chunk_id:
                logger.error(f"é—®é¢˜ {question.question_id} ç¼ºå°‘æºä¿¡æ¯")
                return False

        return True

    def set_custom_prompts(self, system_prompt: str, human_prompt: str) -> None:
        """
        è®¾ç½®è‡ªå®šä¹‰æç¤ºè¯

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
            human_prompt: ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
        """
        self.system_prompt = system_prompt
        self.human_prompt = human_prompt
        logger.info("è‡ªå®šä¹‰æç¤ºè¯å·²æ›´æ–°")

    def _extract_fallback_questions(
        self,
        response: str,
        source_chunk: DocumentChunk,
        context_package: Dict[str, Any],
        start_index: int = 1,
    ) -> List[Question]:
        """
        å½“ç»“æ„åŒ–è§£æå¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æå–é—®é¢˜

        Args:
            response: åŸå§‹å“åº”æ–‡æœ¬
            source_chunk: æºå—

        Returns:
            Questionå¯¹è±¡åˆ—è¡¨
        """
        questions = []

        # æŒ‰è¡Œåˆ†å‰²å¹¶æŸ¥æ‰¾ç±»ä¼¼é—®é¢˜çš„å†…å®¹
        lines = response.split('\n')
        question_index = start_index

        base_related_entities = list(
            dict.fromkeys(context_package.get("related_entities", []))
        )
        base_related_chunk_ids = list(
            dict.fromkeys(context_package.get("related_chunk_ids", []))
        )
        primary_chunk_id = compute_lightrag_chunk_id(source_chunk.content)
        if primary_chunk_id and primary_chunk_id not in base_related_chunk_ids:
            base_related_chunk_ids = [primary_chunk_id] + base_related_chunk_ids
        knowledge_used = bool(context_package.get("prompt_context"))

        for line in lines:
            line = line.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            
            # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆmarkdownæ ‡é¢˜ã€åˆ†ç±»æ ‡è®°ç­‰ï¼‰
            if line.startswith('#') or line.startswith('ã€') or line.startswith('##'):
                continue
            
            # è·³è¿‡é—®é¢˜åˆ†ç±»æ ‡é¢˜
            if re.match(r'^(å¤æ‚|ä¸­ç­‰|ç®€å•|å…³è”|æ·±åº¦|äº‹å®).*é—®é¢˜', line):
                continue
            
            # è·³è¿‡åªåŒ…å«"é—®é¢˜N:"ä½†æ²¡æœ‰å®é™…å†…å®¹çš„è¡Œ
            if re.match(r'^é—®é¢˜\d+[:\ï¼š]\s*$', line):
                continue

            # æŸ¥æ‰¾å¯èƒ½æ˜¯é—®é¢˜çš„è¡Œ - å¿…é¡»åŒ…å«é—®å·æˆ–ä»¥ç–‘é—®è¯å¼€å¤´
            if ('?' in line or 'ï¼Ÿ' in line or
                    line.startswith(('å¦‚ä½•', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€æ ·', 'å“ªäº›', 'æ˜¯å¦', 'èƒ½å¦', 'ä¼šä¸ä¼š'))):

                # æ¸…ç†è¡Œå†…å®¹
                cleaned_line = re.sub(r'^[\d\.\-\*\s]+', '', line)  # ç§»é™¤ç¼–å·
                cleaned_line = re.sub(r'^é—®é¢˜\d+[:\ï¼š]\s*', '', cleaned_line)  # ç§»é™¤"é—®é¢˜N:"å‰ç¼€

                # å¿…é¡»æœ‰å®è´¨å†…å®¹ä¸”åŒ…å«é—®å·
                if len(cleaned_line) > 15 and ('?' in cleaned_line or 'ï¼Ÿ' in cleaned_line):
                    question = self._build_question_object(
                        question_content=cleaned_line,
                        source_chunk=source_chunk,
                        question_index=question_index,
                        base_related_entities=base_related_entities,
                        base_related_chunk_ids=base_related_chunk_ids,
                        primary_chunk_id=primary_chunk_id,
                        knowledge_used=knowledge_used,
                    )
                    questions.append(question)
                    question_index += 1

                    # é™åˆ¶åˆ°é¢„æœŸçš„é—®é¢˜æ•°é‡
                    if len(questions) >= self.questions_per_chunk:
                        break

        return questions
