#!/usr/bin/env python3
"""å¿«é€Ÿæ„å»ºæ”¯æŒæœ¬åœ°æ¨¡å‹çš„å¯æ‰§è¡Œæ–‡ä»¶è„šæœ¬"""

import subprocess
import sys
import os
import shutil
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥å·¥å…·æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent / 'src'))

try:
    from utils.console_utils import ConsoleOutputFixer, safe_print, print_with_emoji
    # ä¿®å¤æ§åˆ¶å°ç¼–ç 
    ConsoleOutputFixer.fix_console_encoding()
    use_safe_print = True
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨æ ‡å‡†print
    def safe_print(*args, **kwargs):
        print(*args, **kwargs)
    def print_with_emoji(emoji, message, level="DEBUG"):
        print(f"{emoji} {message}")
    use_safe_print = False

def main():
    print_with_emoji("ğŸš€", "å¿«é€Ÿæ„å»ºæ”¯æŒæœ¬åœ°æ¨¡å‹çš„å¯æ‰§è¡Œæ–‡ä»¶...")
    
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
    venv_path = Path('build_venv')
    if os.name == 'nt':
        venv_python = venv_path / 'Scripts' / 'python.exe'
    else:
        venv_python = venv_path / 'bin' / 'python'
    
    if not venv_python.exists():
        print_with_emoji("âŒ", "è™šæ‹Ÿç¯å¢ƒä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ python build_with_venv.py")
        sys.exit(1)
    
    # æ„å»ºå‘½ä»¤ - æ·»åŠ æœ¬åœ°æ¨¡å‹ç›¸å…³ä¾èµ–
    cmd = [
        str(venv_python), '-m', 'PyInstaller',
        '--onefile', '--clean', '--noconfirm',
        '--name=qa_gen_pipeline_local',
        '--add-data=config_local.yaml;config.yaml',
        '--add-data=src;src',
        # LightRAGç›¸å…³
        '--hidden-import=lightrag',
        '--hidden-import=lightrag.utils',
        '--hidden-import=lightrag.llm',
        '--hidden-import=lightrag.storage',
        '--hidden-import=lightrag.operate',
        '--hidden-import=lightrag.base',
        '--hidden-import=lightrag.kg',
        '--hidden-import=lightrag.kg.json_kv_impl',
        '--hidden-import=lightrag.kg.neo4j_impl',
        '--hidden-import=lightrag.kg.networkx_impl',
        '--hidden-import=lightrag.kg.nano_vector_db_impl',
        '--hidden-import=lightrag.kg.age_impl',
        '--hidden-import=lightrag.kg.chroma_impl',
        '--hidden-import=lightrag.kg.faiss_impl',
        '--hidden-import=lightrag.kg.gremlin_impl',
        '--hidden-import=lightrag.kg.json_doc_status_impl',
        '--hidden-import=lightrag.kg.milvus_impl',
        '--hidden-import=lightrag.kg.mongo_impl',
        '--hidden-import=lightrag.kg.postgres_impl',
        '--hidden-import=lightrag.kg.qdrant_impl',
        '--hidden-import=lightrag.kg.redis_impl',
        '--hidden-import=lightrag.kg.shared_storage',
        '--hidden-import=lightrag.kg.tidb_impl',
        '--hidden-import=lightrag.graph',
        '--hidden-import=lightrag.memory',
        '--hidden-import=lightrag.retrieve',
        # åŸºç¡€ä¾èµ–
        '--hidden-import=openai',
        '--hidden-import=requests',
        '--hidden-import=loguru',
        '--hidden-import=numpy',
        '--hidden-import=pandas',
        '--hidden-import=networkx',
        '--hidden-import=networkx.algorithms',
        '--hidden-import=networkx.algorithms.community',
        '--hidden-import=graspologic',
        '--hidden-import=tiktoken',
        '--hidden-import=tiktoken.registry',
        '--hidden-import=tiktoken_ext',
        '--hidden-import=tiktoken_ext.openai_public',
        '--collect-data=tiktoken',
        '--hidden-import=nano_vectordb',
        '--hidden-import=nest_asyncio',
        '--hidden-import=jinja2',
        '--hidden-import=markdown',
        '--hidden-import=jsonlines',
        # æœ¬åœ°æ¨¡å‹ç›¸å…³ä¾èµ–
        '--hidden-import=ollama',
        '--hidden-import=vllm',
        '--hidden-import=transformers',
        '--hidden-import=torch',
        '--hidden-import=fastapi',
        '--hidden-import=uvicorn',
        '--hidden-import=pydantic',
        'main.py'
    ]
    
    safe_print("æ‰§è¡Œæ„å»ºå‘½ä»¤...")
    # è·å–å½“å‰ç³»ç»Ÿçš„å®Œæ•´ç¯å¢ƒå˜é‡ï¼Œå¹¶å¤åˆ¶ä¸€ä»½
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print_with_emoji("âœ“", "æ„å»ºæˆåŠŸ")
        # åˆ›å»ºéƒ¨ç½²åŒ…
        print_with_emoji("ğŸ“¦", "åˆ›å»ºæœ¬åœ°æ¨¡å‹éƒ¨ç½²åŒ…...")
        deploy_dir = Path("deployment_local")
        if deploy_dir.exists():
            try:
                shutil.rmtree(deploy_dir)
            except PermissionError:
                safe_print("âš ï¸  deployment_localç›®å½•è¢«å ç”¨ï¼Œä½¿ç”¨æ–°ç›®å½•å...")
                import time
                deploy_dir = Path(f"deployment_local_{int(time.time())}")
                safe_print(f"âœ“ ä½¿ç”¨æ–°ç›®å½•: {deploy_dir}")
        deploy_dir.mkdir()
        
        # å¤åˆ¶å¯æ‰§è¡Œæ–‡ä»¶
        exe_name = "qa_gen_pipeline_local.exe" if os.name == 'nt' else "qa_gen_pipeline_local"
        exe_source = Path("dist") / exe_name
        exe_target = deploy_dir / exe_name
        
        if exe_source.exists():
            shutil.copy2(exe_source, exe_target)
            if os.name != 'nt':
                os.chmod(exe_target, 0o755)
            safe_print(f"âœ“ å¤åˆ¶å¯æ‰§è¡Œæ–‡ä»¶: {exe_name}")
        else:
            print_with_emoji("âŒ", f"å¯æ‰§è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {exe_source}")
            return False
        
        # å¤åˆ¶é…ç½®æ–‡ä»¶
        shutil.copy2("config_local.yaml", deploy_dir / "config.yaml")
        safe_print("âœ“ å¤åˆ¶æœ¬åœ°æ¨¡å‹é…ç½®æ–‡ä»¶")
        
        # åˆ›å»ºæœ¬åœ°æ¨¡å‹ç¯å¢ƒå˜é‡ç¤ºä¾‹
        env_content = """# QAç”Ÿæˆç®¡é“æœ¬åœ°æ¨¡å‹ç¯å¢ƒå˜é‡é…ç½®
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env å¹¶å¡«å…¥å®é™…çš„APIå¯†é’¥

# æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆæ¨èï¼‰
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ—¶ï¼Œä»¥ä¸‹APIå¯†é’¥ä¸æ˜¯å¿…éœ€çš„
# DEEPSEEK_API_KEY=your_deepseek_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here

# æœ¬åœ°æ¨¡å‹æœåŠ¡é…ç½®
# OllamaæœåŠ¡åœ°å€ï¼ˆé»˜è®¤ï¼‰
OLLAMA_BASE_URL=http://localhost:11434

# æœ¬åœ°æ¨¡å‹åç§°
LOCAL_MODEL_NAME=deepseek-r1:32b

# å¯é€‰ï¼švLLMæœåŠ¡é…ç½®
# VLLM_BASE_URL=http://localhost:8000
"""
        with open(deploy_dir / ".env.example", 'w', encoding='utf-8') as f:
            f.write(env_content)
        safe_print("âœ“ åˆ›å»ºæœ¬åœ°æ¨¡å‹ç¯å¢ƒå˜é‡ç¤ºä¾‹")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        for dirname in ["working", "output", "logs", "temp"]:
            (deploy_dir / dirname).mkdir()
            (deploy_dir / dirname / ".gitkeep").touch()
        safe_print("âœ“ åˆ›å»ºç›®å½•ç»“æ„")
        
        # åˆ›å»ºæœ¬åœ°æ¨¡å‹ä½¿ç”¨è¯´æ˜
        readme_content = """# æœ¬åœ°æ¨¡å‹éƒ¨ç½²åŒ…ä½¿ç”¨è¯´æ˜

## ğŸ¯ æ¦‚è¿°
è¿™æ˜¯æ”¯æŒæœ¬åœ°æ¨¡å‹çš„QAç”Ÿæˆç®¡é“å¯æ‰§è¡Œæ–‡ä»¶ï¼Œä½¿ç”¨deepseek-r1:32bæ¨¡å‹è¿›è¡Œé—®ç­”ç”Ÿæˆã€‚

## ğŸ“‹ ä½¿ç”¨å‰å‡†å¤‡

### 1. å®‰è£…Ollama
```bash
# Windows: ä¸‹è½½å¹¶å®‰è£… https://ollama.ai/download
# Linux/macOS:
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. å¯åŠ¨OllamaæœåŠ¡
```bash
ollama serve
```

### 3. ä¸‹è½½æ¨¡å‹
```bash
ollama pull deepseek-r1:32b
```

### 4. æµ‹è¯•æ¨¡å‹
```bash
ollama run deepseek-r1:32b "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. é…ç½®ç¯å¢ƒ
```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡ç¤ºä¾‹
copy .env.example .env

# ç¼–è¾‘.envæ–‡ä»¶ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®å³å¯ï¼‰
```

### 2. è¿è¡Œç¨‹åº
```bash
# Windows
qa_gen_pipeline_local.exe

# Linux/macOS
./qa_gen_pipeline_local
```

## âš™ï¸ é…ç½®è¯´æ˜

### config.yamlé…ç½®
ç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹é…ç½®ï¼š
```yaml
question_generator:
  provider: "local"
  local:
    model_name: "deepseek-r1:32b"
    base_url: "http://localhost:11434"
    max_tokens: 2048
    temperature: 0.7
    timeout: 120
    questions_per_chunk: 30
```

### åˆ‡æ¢å›APIæ¨¡å¼
å¦‚éœ€ä½¿ç”¨APIæ¨¡å¼ï¼Œä¿®æ”¹config.yamlï¼š
```yaml
question_generator:
  provider: "deepseek"  # æ”¹ä¸ºdeepseekä½¿ç”¨API
```

## ğŸ”§ æ•…éšœæ’é™¤

### 1. æ¨¡å‹è¿æ¥å¤±è´¥
- æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œï¼š`ollama serve`
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸‹è½½ï¼š`ollama list`
- æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨ï¼š`netstat -an | grep 11434`

### 2. GPUå†…å­˜ä¸è¶³
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š`ollama pull deepseek-r1:7b`
- æ£€æŸ¥GPUçŠ¶æ€ï¼š`nvidia-smi`

### 3. ç¨‹åºè¿è¡Œç¼“æ…¢
- æ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼š`nvidia-smi`
- è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼šä¿®æ”¹config.yamlä¸­çš„timeoutå€¼

## ğŸ’¡ ä¼˜åŠ¿

- âœ… å®Œå…¨å…è´¹ï¼Œæ— APIè´¹ç”¨
- âœ… æ•°æ®å®‰å…¨ï¼Œä¸ç¦»å¼€æœ¬åœ°
- âœ… å“åº”é€Ÿåº¦å¿«ï¼Œæ— ç½‘ç»œå»¶è¿Ÿ
- âœ… æ— ä½¿ç”¨é™åˆ¶
- âœ… å®Œå…¨ç¦»çº¿è¿è¡Œ

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. OllamaæœåŠ¡çŠ¶æ€
2. æ¨¡å‹ä¸‹è½½æƒ…å†µ
3. GPUå†…å­˜ä½¿ç”¨
4. é…ç½®æ–‡ä»¶è®¾ç½®

äº«å—å…è´¹çš„æœ¬åœ°AIæœåŠ¡ï¼ğŸ‰
"""
        with open(deploy_dir / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)
        safe_print("âœ“ åˆ›å»ºä½¿ç”¨è¯´æ˜")
        
        # åˆ›å»ºå¯åŠ¨è„šæœ¬
        if os.name == 'nt':
            # Windowsæ‰¹å¤„ç†è„šæœ¬
            bat_content = """@echo off
echo å¯åŠ¨QAç”Ÿæˆç®¡é“ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆæœ¬ï¼‰...
echo.
echo è¯·ç¡®ä¿ï¼š
echo 1. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)
echo 2. deepseek-r1:32bæ¨¡å‹å·²ä¸‹è½½ (ollama pull deepseek-r1:32b)
echo.
pause
qa_gen_pipeline_local.exe
pause
"""
            with open(deploy_dir / "start.bat", 'w', encoding='utf-8') as f:
                f.write(bat_content)
            safe_print("âœ“ åˆ›å»ºWindowså¯åŠ¨è„šæœ¬")
        else:
            # Linux/macOS shellè„šæœ¬
            sh_content = """#!/bin/bash
echo "å¯åŠ¨QAç”Ÿæˆç®¡é“ï¼ˆæœ¬åœ°æ¨¡å‹ç‰ˆæœ¬ï¼‰..."
echo ""
echo "è¯·ç¡®ä¿ï¼š"
echo "1. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)"
echo "2. deepseek-r1:32bæ¨¡å‹å·²ä¸‹è½½ (ollama pull deepseek-r1:32b)"
echo ""
read -p "æŒ‰å›è½¦é”®ç»§ç»­..."
./qa_gen_pipeline_local
"""
            with open(deploy_dir / "start.sh", 'w', encoding='utf-8') as f:
                f.write(sh_content)
            os.chmod(deploy_dir / "start.sh", 0o755)
            safe_print("âœ“ åˆ›å»ºLinux/macOSå¯åŠ¨è„šæœ¬")
        
        safe_print("")
        print_with_emoji("ğŸ‰", "æœ¬åœ°æ¨¡å‹æ„å»ºå®Œæˆ!")
        safe_print(f"âœ“ å¯æ‰§è¡Œæ–‡ä»¶: deployment_local/{exe_name}")
        safe_print("âœ“ åŒ…å«å®Œæ•´çš„æœ¬åœ°æ¨¡å‹æ”¯æŒ")
        safe_print("âœ“ åŒ…å«ä½¿ç”¨è¯´æ˜å’Œå¯åŠ¨è„šæœ¬")
        safe_print("")
        safe_print("ğŸ“‹ ä½¿ç”¨æ­¥éª¤:")
        safe_print("1. å®‰è£…Ollama: https://ollama.ai/download")
        safe_print("2. å¯åŠ¨æœåŠ¡: ollama serve")
        safe_print("3. ä¸‹è½½æ¨¡å‹: ollama pull deepseek-r1:32b")
        safe_print("4. è¿è¡Œç¨‹åº: deployment_local/start.bat (Windows) æˆ– ./start.sh (Linux/macOS)")
        
    else:
        print_with_emoji("âŒ", f"æ„å»ºå¤±è´¥: {result.stderr}")
        if result.stdout:
            safe_print(f"æ ‡å‡†è¾“å‡º: {result.stdout}")
        sys.exit(1)

if __name__ == "__main__":
    main()
