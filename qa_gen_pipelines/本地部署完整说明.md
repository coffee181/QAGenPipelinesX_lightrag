# QA生成管道本地部署完整说明

## 概述

本文档详细说明了QA生成管道的本地部署方案，包括所有相关Python文件的功能说明和使用方法。该方案使用Ollama运行DeepSeek R1模型，结合LightRAG进行知识库管理和答案生成。

## 核心架构

```
qa_gen_pipelines/
├── main.py                           # 主程序入口
├── config_local.yaml                 # 本地部署配置文件
├── quick_build_local.py             # 本地模型构建脚本
├── src/
│   ├── implementations/              # 核心实现模块
│   │   ├── local_question_generator.py    # 本地问题生成器
│   │   ├── lightrag_rag.py               # LightRAG知识库管理
│   │   ├── simple_markdown_processor.py  # 文本清理处理器
│   │   ├── simple_text_chunker.py        # 文本分块器
│   │   └── tesseract_ocr.py              # OCR处理器
│   ├── services/                     # 服务层
│   │   ├── answer_service.py             # 答案生成服务
│   │   ├── progress_manager.py           # 进度管理器
│   │   └── question_service.py           # 问题生成服务
│   ├── models/                       # 数据模型
│   │   ├── document.py                   # 文档模型
│   │   ├── question.py                   # 问题模型
│   │   └── qa_pair.py                     # 问答对模型
│   ├── interfaces/                  # 接口定义
│   └── utils/                       # 工具类
└── timeout_config.py               # 超时配置
```

## 核心Python文件说明

### 1. 主程序文件

#### `main.py` - 主程序入口
- **功能**: 应用程序的主入口点，负责服务创建和命令执行
- **关键特性**:
  - 支持本地模型和云端模型的选择
  - 集成问题生成和答案生成流程
  - 提供命令行接口

#### `config_local.yaml` - 本地部署配置
- **功能**: 本地部署的配置文件
- **关键配置**:
  ```yaml
  question_generator:
    provider: "local"                    # 使用本地模型
    local:
      model_name: "deepseek-r1:32b"      # Ollama模型名称
      base_url: "http://localhost:11434" # Ollama服务地址
      timeout: 30000                     # 超时设置
      questions_per_chunk: 10           # 每个文本块生成的问题数
  
  rag:
    lightrag:
      openai:
        api_key: "your-openai-key"       # OpenAI API密钥（用于embedding）
  
  prompts:
    system_prompt: "专业的问题生成提示词"
    human_prompt: "基于文档内容生成问题的提示词"
  ```

### 2. 核心实现模块

#### `local_question_generator.py` - 本地问题生成器
- **功能**: 基于Ollama的本地问题生成
- **关键特性**:
  - 支持DeepSeek R1模型
  - 自动清理`<think>`标签
  - 超时重试机制
  - 详细的日志记录

- **主要方法**:
  ```python
  def generate_questions_from_chunk(self, chunk: DocumentChunk) -> List[Question]
  def _call_ollama_api(self, prompt: str) -> str
  def _clean_think_tags(self, text: str) -> str
  ```

#### `lightrag_rag.py` - LightRAG知识库管理
- **功能**: 基于LightRAG的知识库管理和答案生成
- **关键特性**:
  - 支持文档插入和向量化
  - 自定义提示词（移除参考资料要求）
  - 支持naive和local模式
  - 集成OpenAI embedding

- **主要方法**:
  ```python
  def insert_document(self, document: Document) -> None
  def query_single_question(self, question: str) -> str
  def _clean_think_tags(self, text: str) -> str
  ```

#### `simple_markdown_processor.py` - 文本清理处理器
- **功能**: 清理LLM响应中的格式和不需要的内容
- **关键特性**:
  - 移除`<think>`标签
  - 清理参考资料部分
  - 移除markdown格式
  - 清理LLM响应中的各种artifacts

- **主要方法**:
  ```python
  def clean_llm_response(self, response: str) -> str
  def _clean_llm_artifacts(self, text: str) -> str
  ```

#### `simple_text_chunker.py` - 文本分块器
- **功能**: 将长文档分割成适合处理的文本块
- **关键特性**:
  - 智能分块算法
  - 保持语义完整性
  - 可配置的块大小

#### `tesseract_ocr.py` - OCR处理器
- **功能**: 从PDF文档中提取文本
- **关键特性**:
  - 支持多种PDF格式
  - 中文文本识别优化
  - 错误处理和重试

### 3. 服务层文件

#### `answer_service.py` - 答案生成服务
- **功能**: 管理答案生成和QA对创建
- **关键特性**:
  - 批量答案生成
  - 增量保存机制
  - 答案质量验证
  - 详细的进度日志

- **主要方法**:
  ```python
  def generate_answers_for_questions(self, questions_file: Path, output_file: Path, session_id: str) -> QASet
  def _generate_answers_batch(self, questions: List[Question], session_id: str) -> QASet
  def _is_answer_invalid(self, answer: str) -> bool
  ```

#### `progress_manager.py` - 进度管理器
- **功能**: 管理处理进度和状态
- **关键特性**:
  - 实时进度跟踪
  - 会话状态管理
  - 增量保存支持

#### `question_service.py` - 问题生成服务
- **功能**: 管理问题生成流程
- **关键特性**:
  - 批量问题生成
  - 进度跟踪
  - 错误处理

### 4. 数据模型文件

#### `document.py` - 文档模型
- **功能**: 定义文档相关的数据结构
- **关键类**:
  ```python
  class Document
  class DocumentChunk
  ```

#### `question.py` - 问题模型
- **功能**: 定义问题相关的数据结构
- **关键类**:
  ```python
  class Question
  class QuestionSet
  ```

#### `qa_pair.py` - 问答对模型
- **功能**: 定义问答对的数据结构
- **关键类**:
  ```python
  class QAPair
  class QASet
  ```

### 5. 工具文件

#### `timeout_config.py` - 超时配置
- **功能**: 集中管理超时设置
- **关键特性**:
  - 全局超时配置
  - Ollama特定超时设置
  - 重试机制配置

## 部署和使用说明

### 1. 环境准备

#### 安装Ollama
```bash
# 下载并安装Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 启动Ollama服务
ollama serve

# 下载DeepSeek R1模型
ollama pull deepseek-r1:32b
```

#### 安装Python依赖
```bash
pip install -r requirements.txt
```

### 2. 配置设置

#### 修改配置文件
编辑`config_local.yaml`:
```yaml
question_generator:
  provider: "local"
  local:
    model_name: "deepseek-r1:32b"
    base_url: "http://localhost:11434"
    timeout: 30000

rag:
  lightrag:
    openai:
      api_key: "your-openai-api-key"  # 设置OpenAI API密钥
```

### 3. 构建可执行文件

#### 使用构建脚本
```bash
python quick_build_local.py
```

#### 构建脚本功能
- 自动检测依赖
- 打包所有必要文件
- 创建独立的可执行文件
- 包含配置文件

### 4. 使用方法

#### 基本使用流程

1. **插入文档到知识库**
```bash
python main.py insert-documents --documents ./example_pdfs --working-dir ./knowledge_base
```

2. **生成问题**
```bash
python main.py generate-questions --documents ./example_pdfs --output ./questions.jsonl
```

3. **生成答案**
```bash
python main.py generate-answers --questions ./questions.jsonl --working-dir ./knowledge_base --output ./qa_pairs.jsonl
```

#### 高级使用

1. **批量处理多个文档**
```bash
python main.py generate-answers --insert-documents ./documents --questions ./questions.jsonl --working-dir ./kb --output ./results.jsonl
```

2. **使用现有知识库**
```bash
python main.py generate-answers --questions ./questions.jsonl --working-dir ./existing_kb --output ./results.jsonl
```

### 5. 输出格式

#### 问题文件格式 (questions.jsonl)
```json
{"question_id": "uuid", "content": "问题内容", "source_document": "文档ID", "source_chunk_id": "块ID", "question_index": 1, "created_at": "时间戳"}
```

#### 问答对文件格式 (qa_pairs.jsonl)
```json
{"question_id": "uuid", "question": "问题内容", "answer": "答案内容", "source_document": "文档ID", "confidence_score": 1.0, "metadata": {...}}
```

### 6. 故障排除

#### 常见问题

1. **Ollama连接失败**
   - 检查Ollama服务是否启动
   - 验证端口11434是否可用
   - 确认模型是否已下载

2. **超时问题**
   - 增加`timeout`配置值
   - 检查网络连接
   - 确认模型性能

3. **内存不足**
   - 减少`questions_per_chunk`数量
   - 使用更小的模型
   - 增加系统内存

#### 日志查看
```bash
# 查看详细日志
tail -f qa_pipeline.log

# 查看错误日志
grep "ERROR" qa_pipeline.log
```

### 7. 性能优化

#### 配置优化
- 调整`questions_per_chunk`参数
- 优化超时设置
- 配置合适的批处理大小

#### 硬件要求
- **CPU**: 8核心以上推荐
- **内存**: 16GB以上推荐
- **存储**: SSD推荐
- **GPU**: 可选，用于加速推理

### 8. 高级功能

#### 自定义提示词
修改`config_local.yaml`中的提示词配置：
```yaml
prompts:
  system_prompt: "你的自定义系统提示词"
  human_prompt: "你的自定义用户提示词"
```

#### 自定义清理规则
在`simple_markdown_processor.py`中添加自定义清理规则：
```python
def _clean_custom_artifacts(self, text: str) -> str:
    # 添加你的自定义清理逻辑
    return text
```

## 技术特点

### 1. 本地化部署
- 完全本地运行，无需云端API
- 数据隐私保护
- 无网络依赖

### 2. 智能文本处理
- 自动清理思考过程
- 移除参考资料部分
- 保持内容质量

### 3. 高效处理
- 批量处理支持
- 增量保存机制
- 进度实时跟踪

### 4. 可扩展性
- 模块化设计
- 接口标准化
- 易于扩展功能

## 注意事项

1. **首次运行**: 需要下载模型，可能需要较长时间
2. **内存使用**: DeepSeek R1模型需要大量内存
3. **处理时间**: 本地模型处理速度相对较慢
4. **质量保证**: 建议在正式使用前进行测试

## 支持与维护

- 查看日志文件进行问题诊断
- 定期更新模型和依赖
- 监控系统资源使用情况
- 备份重要的知识库数据

---

*本文档涵盖了QA生成管道本地部署的所有核心组件和使用方法。如有问题，请参考日志文件或联系技术支持。*
