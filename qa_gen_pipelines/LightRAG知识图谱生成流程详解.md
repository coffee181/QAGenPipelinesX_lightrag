# LightRAG çŸ¥è¯†å›¾è°±ç”Ÿæˆæµç¨‹è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

LightRAG æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ£€ç´¢ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿä»æ–‡æ¡£ä¸­è‡ªåŠ¨æå–å®ä½“ã€å…³ç³»å’ŒçŸ¥è¯†ï¼Œæ„å»ºç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ã€‚

## ğŸ”§ æ ¸å¿ƒç»„ä»¶æ¶æ„

### 1. LightRAG å®ä¾‹åˆå§‹åŒ–

```python
# src/implementations/lightrag_rag.py:163-335
def _create_lightrag_instance(self):
    """Create LightRAG instance with proper configuration."""
    
    # 1. å®šä¹‰å¼‚æ­¥LLMå‡½æ•°
    async def llm_model_func(prompt, system_prompt=None, history_messages=None, **kwargs):
        """LLM function for LightRAG using local Ollama model."""
        # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ (deepseek-r1:32b)
        ollama_url = "http://localhost:11434/api/generate"
        model_name = "deepseek-r1:32b"
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        full_prompt = ""
        if system_prompt:
            full_prompt += f"{system_prompt}\n\n"
        
        # å¤„ç†å†å²æ¶ˆæ¯
        if history_messages:
            # æ ¼å¼åŒ–å†å²å¯¹è¯
            for msg in history_messages:
                if isinstance(msg, dict) and "role" in msg and "content" in msg:
                    role = msg["role"]
                    content = msg["content"]
                    if role == "system":
                        full_prompt += f"System: {content}\n\n"
                    elif role == "user":
                        full_prompt += f"User: {content}\n\n"
                    elif role == "assistant":
                        full_prompt += f"Assistant: {content}\n\n"
        
        # æ·»åŠ å½“å‰æç¤º
        full_prompt += f"User: {prompt}\n\nAssistant:"
        
        # å‡†å¤‡Ollamaè¯·æ±‚
        payload = {
            "model": model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "num_predict": kwargs.get("max_tokens", 2048)
            }
        }
        
        # é‡è¯•æœºåˆ¶ (æœ€å¤š5æ¬¡)
        max_retries = 5
        retry_delay = 5
        
        for attempt in range(max_retries):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        ollama_url,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=1800)  # 30åˆ†é’Ÿè¶…æ—¶
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            raw_response = result.get("response", "")
                            # æ¸…ç†<think>æ ‡ç­¾
                            cleaned_response = self._clean_think_tags(raw_response)
                            return cleaned_response
                        else:
                            # é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
                            error_text = await response.text()
                            logger.error(f"Ollama API error {response.status}: {error_text}")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(retry_delay)
                                retry_delay *= 2
                                continue
                            else:
                                raise Exception(f"Ollama API error {response.status}: {error_text}")
                                
            except asyncio.TimeoutError:
                logger.error(f"Ollama API timeout on attempt {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    raise Exception("Ollama API timeout after all retries")
                    
            except Exception as e:
                logger.error(f"Unexpected error in Ollama LLM function: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    raise

    # 2. å®šä¹‰å¼‚æ­¥åµŒå…¥å‡½æ•°
    async def embedding_func(texts: List[str]):
        """Embedding function for LightRAG."""
        import numpy as np

        if self.openai_api_key:
            try:
                import openai
                # ä½¿ç”¨OpenAIåµŒå…¥
                client = openai.AsyncOpenAI(api_key=self.openai_api_key)
                response = await client.embeddings.create(
                    model="text-embedding-3-large",  # ä½¿ç”¨å¤§æ¨¡å‹ï¼Œ3072ç»´
                    input=texts
                )
                embeddings = [data.embedding for data in response.data]
                return np.array(embeddings)
            except Exception as e:
                logger.warning(f"OpenAI embedding failed: {e}, using fallback")

        # ç®€å•å›é€€ - åˆ›å»º3072ç»´åµŒå…¥
        import hashlib
        embeddings = []
        for text in texts:
            hash_obj = hashlib.md5(text.encode())
            hash_int = int(hash_obj.hexdigest(), 16)
            embedding = [(hash_int >> i) & 1 for i in range(3072)]  # 3072ç»´
            embeddings.append(embedding)
        return np.array(embeddings, dtype=np.float32)

    # 3. åˆ›å»ºLightRAGå®ä¾‹
    try:
        rag = LightRAG(
            working_dir=str(self.working_dir),
            llm_model_func=llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=3072,  # åŒ¹é…ç°æœ‰çŸ¥è¯†åº“
                max_token_size=8192,
                func=embedding_func
            ),
            # ä½¿ç”¨å…¼å®¹ç¼–ç 
            encoding_model="cl100k_base"  # ä½¿ç”¨cl100k_baseè€Œä¸æ˜¯o200k_base
        )
    except TypeError:
        # å¦‚æœencoding_modelå‚æ•°ä¸æ”¯æŒï¼Œå°è¯•ä¸ä½¿ç”¨å®ƒ
        rag = LightRAG(
            working_dir=str(self.working_dir),
            llm_model_func=llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=3072,  # åŒ¹é…ç°æœ‰çŸ¥è¯†åº“
                max_token_size=8192,
                func=embedding_func
            )
        )

    # 4. åˆå§‹åŒ–å­˜å‚¨
    try:
        async def initialize_all():
            await rag.initialize_storages()
            try:
                from lightrag.kg.shared_storage import initialize_pipeline_status
                await initialize_pipeline_status()
            except ImportError:
                pass

        asyncio.run(initialize_all())
    except Exception as e:
        logger.error(f"FATAL: Failed to initialize LightRAG storages: {e}")
        raise RAGError(f"Failed to initialize LightRAG storages: {e}")

    return rag
```

## ğŸ”„ çŸ¥è¯†å›¾è°±ç”Ÿæˆæµç¨‹

### ç¬¬1æ­¥ï¼šæ–‡æ¡£æ’å…¥å‡†å¤‡

```python
# src/implementations/lightrag_rag.py:359-378
def insert_document(self, document: Document) -> None:
    """
    Insert a single document into the knowledge base.
    
    Args:
        document: Document to insert
    
    Raises:
        RAGError: If insertion fails
    """
    try:
        logger.info(f"Inserting document: {document.name}")
        
        # ä½¿ç”¨å¼‚æ­¥è¾…åŠ©å‡½æ•°
        asyncio.run(self._async_insert_document(document))
        
        logger.info(f"Successfully inserted document: {document.name}")
        
    except Exception as e:
        raise RAGError(f"Failed to insert document {document.name}: {e}")
```

### ç¬¬2æ­¥ï¼šå¼‚æ­¥æ–‡æ¡£å¤„ç†

```python
# src/implementations/lightrag_rag.py:380-408
async def _async_insert_document(self, document: Document) -> None:
    """
    Async helper for inserting documents.
    
    Args:
        document: Document to insert
    """
    # ç¡®ä¿å­˜å‚¨å·²åˆå§‹åŒ–
    try:
        await self.rag.initialize_storages()
        
        # å¦‚æœå¯ç”¨ï¼Œåˆå§‹åŒ–ç®¡é“çŠ¶æ€
        try:
            from lightrag.kg.shared_storage import initialize_pipeline_status
            await initialize_pipeline_status()
        except ImportError:
            pass  # å¹¶éæ‰€æœ‰ç‰ˆæœ¬éƒ½å¯ç”¨
    except Exception as e:
        logger.warning(f"Storage initialization warning: {e}")
    
    # æ’å…¥æ–‡æ¡£
    try:
        await self.rag.ainsert(document.content)
    except Exception as e:
        if "history_messages" in str(e):
            logger.warning(f"LightRAG history_messages issue, this is a known problem with current version")
            raise RAGError(f"LightRAG version issue: {e}")
        else:
            raise e
```

## ğŸ§  LightRAG å†…éƒ¨çŸ¥è¯†å›¾è°±æ„å»ºæœºåˆ¶

### 1. æ–‡æœ¬åˆ†å—ä¸é¢„å¤„ç†

LightRAG å†…éƒ¨ä¼šå°†æ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†å—ï¼š

```python
# LightRAG å†…éƒ¨å¤„ç†æµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
def process_document_content(content: str):
    """
    LightRAG å†…éƒ¨æ–‡æ¡£å¤„ç†æµç¨‹
    """
    # 1. æ–‡æœ¬åˆ†å—
    chunks = split_text_into_chunks(content, chunk_size=1000)
    
    # 2. å®ä½“æå–
    entities = extract_entities_from_chunks(chunks)
    
    # 3. å…³ç³»è¯†åˆ«
    relations = identify_relations_between_entities(entities)
    
    # 4. çŸ¥è¯†å›¾è°±æ„å»º
    knowledge_graph = build_knowledge_graph(entities, relations)
    
    return knowledge_graph
```

### 2. å®ä½“æå–è¿‡ç¨‹

```python
# LightRAG å®ä½“æå–æœºåˆ¶ï¼ˆæ¦‚å¿µæ€§ï¼‰
async def extract_entities_from_text(text: str, llm_func):
    """
    ä½¿ç”¨LLMä»æ–‡æœ¬ä¸­æå–å®ä½“
    """
    entity_extraction_prompt = """
    ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰é‡è¦çš„å®ä½“ï¼ˆäººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€æ¦‚å¿µã€æŠ€æœ¯æœ¯è¯­ç­‰ï¼‰ï¼š
    
    æ–‡æœ¬ï¼š{text}
    
    è¯·ä»¥JSONæ ¼å¼è¿”å›å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«ï¼š
    - entity_name: å®ä½“åç§°
    - entity_type: å®ä½“ç±»å‹
    - description: å®ä½“æè¿°
    - context: å‡ºç°ä¸Šä¸‹æ–‡
    """
    
    response = await llm_func(entity_extraction_prompt.format(text=text))
    entities = parse_entity_response(response)
    return entities
```

### 3. å…³ç³»è¯†åˆ«è¿‡ç¨‹

```python
# LightRAG å…³ç³»è¯†åˆ«æœºåˆ¶ï¼ˆæ¦‚å¿µæ€§ï¼‰
async def identify_relations_between_entities(entities: List[Entity], llm_func):
    """
    è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»
    """
    relation_extraction_prompt = """
    åˆ†æä»¥ä¸‹å®ä½“ä¹‹é—´çš„å…³ç³»ï¼š
    
    å®ä½“åˆ—è¡¨ï¼š{entities}
    
    è¯·è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»ï¼ŒåŒ…æ‹¬ï¼š
    - å…³ç³»ç±»å‹ï¼ˆå±äºã€åŒ…å«ã€å½±å“ã€ä¾èµ–ç­‰ï¼‰
    - å…³ç³»å¼ºåº¦
    - å…³ç³»æè¿°
    
    ä»¥JSONæ ¼å¼è¿”å›å…³ç³»åˆ—è¡¨ã€‚
    """
    
    response = await llm_func(relation_extraction_prompt.format(entities=entities))
    relations = parse_relation_response(response)
    return relations
```

### 4. çŸ¥è¯†å›¾è°±å­˜å‚¨ç»“æ„

LightRAG å°†çŸ¥è¯†å›¾è°±å­˜å‚¨ä¸ºä»¥ä¸‹ç»“æ„ï¼š

```python
# LightRAG çŸ¥è¯†å›¾è°±å­˜å‚¨ç»“æ„
knowledge_graph_structure = {
    "entities": {
        "entity_id": {
            "name": "å®ä½“åç§°",
            "type": "å®ä½“ç±»å‹",
            "description": "å®ä½“æè¿°",
            "properties": {
                "å±æ€§1": "å€¼1",
                "å±æ€§2": "å€¼2"
            },
            "embeddings": [0.1, 0.2, ...],  # 3072ç»´å‘é‡
            "created_at": "2024-01-01T00:00:00"
        }
    },
    "relations": {
        "relation_id": {
            "source_entity": "æºå®ä½“ID",
            "target_entity": "ç›®æ ‡å®ä½“ID",
            "relation_type": "å…³ç³»ç±»å‹",
            "description": "å…³ç³»æè¿°",
            "confidence": 0.95,
            "created_at": "2024-01-01T00:00:00"
        }
    },
    "documents": {
        "doc_id": {
            "content": "æ–‡æ¡£å†…å®¹",
            "chunks": ["å—1", "å—2", ...],
            "entities": ["å®ä½“ID1", "å®ä½“ID2", ...],
            "created_at": "2024-01-01T00:00:00"
        }
    }
}
```

## ğŸ” çŸ¥è¯†å›¾è°±æŸ¥è¯¢æœºåˆ¶

### 1. æŸ¥è¯¢æ¨¡å¼

LightRAG æ”¯æŒå¤šç§æŸ¥è¯¢æ¨¡å¼ï¼š

```python
# src/implementations/lightrag_rag.py:486-614
def query_single_question(self, question: str) -> str:
    """
    Query the knowledge base with a single question.
    """
    try:
        logger.info(f"Querying question: {question[:100]}...")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache:
            cached_response = self._check_cache(question)
            if cached_response:
                self.cache_hits += 1
                logger.info(f"âœ… Cache hit! (total hits: {self.cache_hits}, misses: {self.cache_misses})")
                return cached_response
            else:
                self.cache_misses += 1
        
        # ä½¿ç”¨ç°æœ‰äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        response = None
        
        # ä½¿ç”¨naiveæ¨¡å¼è·å¾—æœ€ä½³ç»“æœ
        try:
            logger.info("Using naive mode for reliable results...")
            
            # è‡ªå®šä¹‰æç¤ºè¯ï¼Œè¦æ±‚ç®€æ´ç›´æ¥çš„ç­”æ¡ˆ
            custom_prompt = """---Role---
You are a professional technical documentation assistant. Your role is to provide direct, concise answers based on the provided knowledge base.

---Goal---
Generate a DIRECT and CONCISE answer to the user's question based ONLY on the Knowledge Base provided below.

CRITICAL REQUIREMENTS:
1. Answer DIRECTLY without any preambles like "æ ¹æ®æä¾›çš„çŸ¥è¯†åº“" or "Based on the provided information"
2. Do NOT include reasoning process or explanation of how you found the answer
3. Do NOT mention the knowledge base, documents, or data sources in your answer
4. Simply state the facts as if you're directly reading from a manual
5. Keep the answer focused and to-the-point

---Conversation History---
{history}

---Knowledge Graph and Document Chunks---
{context_data}

---Response Rules---
- Answer format: DIRECT and CONCISE, as if reading from the source document
- NO preambles like "æ ¹æ®", "åŸºäº", "According to", "Based on"
- NO meta-commentary about the knowledge base or your search process
- Use the same language as the user's question
- If the answer involves numbers, specifications, or technical details, state them directly
- If you don't have the information, simply say you cannot find the specific information
- Do NOT make up any information not in the Knowledge Base

Response:"""
            
            # æ·»åŠ è¶…æ—¶é˜²æ­¢æŒ‚èµ·æŸ¥è¯¢
            response = loop.run_until_complete(
                asyncio.wait_for(
                    self.rag.aquery(question, param=QueryParam(mode="naive"), system_prompt=custom_prompt),
                    timeout=30.0  # 30ç§’è¶…æ—¶
                )
            )
            logger.info("Query completed with naive mode")
            
        except asyncio.TimeoutError:
            logger.warning("Query timed out after 30 seconds")
            response = "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–ç®€åŒ–é—®é¢˜ã€‚"
            
        except Exception as e:
            logger.warning(f"Naive mode failed: {e}")
            # å°è¯•localæ¨¡å¼ä½œä¸ºæœ€åæ‰‹æ®µ
            try:
                logger.info("Trying local mode as fallback...")
                response = loop.run_until_complete(
                    asyncio.wait_for(
                        self.rag.aquery(question, param=QueryParam(mode="local"), system_prompt=custom_prompt),
                        timeout=15.0  # æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
                    )
                )
                logger.info("Query completed with local mode")
            except Exception:
                response = "æŠ±æ­‰ï¼Œæ— æ³•ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        
        if response is None:
            response = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
        
        logger.info(f"Generated answer: {len(response)} characters")
        
        # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œç¼“å­˜å“åº”
        if self.enable_cache:
            self._update_cache(question, response)
        
        return response
        
    except Exception as e:
        raise RAGError(f"Failed to query question: {e}")
```

### 2. æŸ¥è¯¢æ¨¡å¼è¯¦è§£

LightRAG æ”¯æŒä¸‰ç§æŸ¥è¯¢æ¨¡å¼ï¼š

#### **Naive æ¨¡å¼**ï¼ˆæ¨èï¼‰
- **ç‰¹ç‚¹**ï¼šç›´æ¥åŸºäºæ–‡æ¡£å—è¿›è¡Œæ£€ç´¢å’Œå›ç­”
- **ä¼˜åŠ¿**ï¼šç»“æœå¯é ï¼Œé€Ÿåº¦å¿«
- **é€‚ç”¨åœºæ™¯**ï¼šå¤§å¤šæ•°æŸ¥è¯¢åœºæ™¯

#### **Local æ¨¡å¼**ï¼ˆå¤‡ç”¨ï¼‰
- **ç‰¹ç‚¹**ï¼šåŸºäºæœ¬åœ°çŸ¥è¯†å›¾è°±è¿›è¡ŒæŸ¥è¯¢
- **ä¼˜åŠ¿**ï¼šèƒ½å¤Ÿåˆ©ç”¨å®ä½“å…³ç³»
- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å…³ç³»æ¨ç†çš„å¤æ‚æŸ¥è¯¢

#### **Global æ¨¡å¼**ï¼ˆå½“å‰ç‰ˆæœ¬æœ‰é—®é¢˜ï¼‰
- **ç‰¹ç‚¹**ï¼šåŸºäºå…¨å±€çŸ¥è¯†å›¾è°±è¿›è¡ŒæŸ¥è¯¢
- **é—®é¢˜**ï¼šå½“å‰ç‰ˆæœ¬å­˜åœ¨å…¼å®¹æ€§é—®é¢˜
- **çŠ¶æ€**ï¼šæš‚æ—¶ç¦ç”¨

## ğŸ“Š çŸ¥è¯†å›¾è°±å­˜å‚¨ä¸ç¼“å­˜

### 1. å­˜å‚¨ç»“æ„

```python
# LightRAG å·¥ä½œç›®å½•ç»“æ„
working_directory/
â”œâ”€â”€ entities.json          # å®ä½“å­˜å‚¨
â”œâ”€â”€ relations.json         # å…³ç³»å­˜å‚¨
â”œâ”€â”€ documents.json         # æ–‡æ¡£å­˜å‚¨
â”œâ”€â”€ chunks.json           # æ–‡æ¡£å—å­˜å‚¨
â”œâ”€â”€ embeddings.json       # åµŒå…¥å‘é‡å­˜å‚¨
â”œâ”€â”€ knowledge_graph.graphml # çŸ¥è¯†å›¾è°±æ–‡ä»¶
â””â”€â”€ pipeline_status.json  # ç®¡é“çŠ¶æ€
```

### 2. ç¼“å­˜æœºåˆ¶

```python
# src/implementations/lightrag_rag.py:808-850
def _check_cache(self, question: str) -> Optional[str]:
    """
    Check if a similar question exists in cache.
    """
    normalized = self._normalize_question(question)
    
    # é¦–å…ˆæ£€æŸ¥å®Œå…¨åŒ¹é…
    if normalized in self.retrieval_cache:
        return self.retrieval_cache[normalized]
    
    # æ£€æŸ¥ç›¸ä¼¼é—®é¢˜
    for cached_question, cached_response in self.retrieval_cache.items():
        similarity = self._calculate_question_similarity(normalized, cached_question)
        if similarity >= self.cache_similarity_threshold:
            logger.debug(f"Found similar cached question (similarity={similarity:.2f})")
            return cached_response
    
    return None

def _update_cache(self, question: str, response: str) -> None:
    """
    Update cache with new question-response pair.
    """
    normalized = self._normalize_question(question)
    self.retrieval_cache[normalized] = response
    
    # é™åˆ¶ç¼“å­˜å¤§å°é˜²æ­¢å†…å­˜é—®é¢˜
    max_cache_size = 1000
    if len(self.retrieval_cache) > max_cache_size:
        # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆFIFOï¼‰
        oldest_key = next(iter(self.retrieval_cache))
        del self.retrieval_cache[oldest_key]
        logger.debug(f"Cache size limit reached, removed oldest entry")

def _calculate_question_similarity(self, text1: str, text2: str) -> float:
    """
    Calculate similarity between two questions using character n-grams.
    """
    def get_ngrams(text, n=2):
        return set(text[i:i+n] for i in range(len(text)-n+1))
    
    ngrams1 = get_ngrams(text1)
    ngrams2 = get_ngrams(text2)
    
    if not ngrams1 or not ngrams2:
        return 0.0
    
    intersection = ngrams1 & ngrams2
    union = ngrams1 | ngrams2
    
    return len(intersection) / len(union) if union else 0.0
```

## ğŸš€ æ‰¹é‡å¤„ç†æœºåˆ¶

### 1. æ‰¹é‡æ–‡æ¡£æ’å…¥

```python
# src/implementations/lightrag_rag.py:410-433
def insert_documents_batch(self, documents: List[Document]) -> None:
    """
    Insert multiple documents into the knowledge base.
    """
    try:
        logger.info(f"Inserting {len(documents)} documents in batch")
        
        for document in tqdm(documents, desc="Inserting documents"):
            try:
                self.insert_document(document)
            except Exception as e:
                logger.error(f"Failed to insert document {document.name}: {e}")
                continue
        
        logger.info(f"Batch insertion completed")
        
    except Exception as e:
        raise RAGError(f"Batch insertion failed: {e}")
```

### 2. ç›®å½•æ‰¹é‡å¤„ç†

```python
# src/implementations/lightrag_rag.py:435-484
def insert_from_directory(self, directory_path: Path) -> None:
    """
    Insert all text files from a directory.
    """
    try:
        if not directory_path.exists():
            raise RAGError(f"Directory does not exist: {directory_path}")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
        text_files = list(directory_path.glob("*.txt"))
        
        if not text_files:
            logger.warning(f"No text files found in directory: {directory_path}")
            return
        
        logger.info(f"Found {len(text_files)} text files in directory: {directory_path}")
        
        for text_file in tqdm(text_files, desc="Processing text files"):
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(text_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                document = Document(
                    file_path=text_file,
                    content=content,
                    file_type=text_file.suffix,
                    file_size=text_file.stat().st_size,
                    created_at=datetime.fromtimestamp(text_file.stat().st_ctime),
                    processed_at=datetime.now()
                )
                
                # æ’å…¥æ–‡æ¡£
                self.insert_document(document)
                
            except Exception as e:
                logger.error(f"Failed to process file {text_file}: {e}")
                continue
        
        logger.info(f"Directory insertion completed")
        
    except Exception as e:
        raise RAGError(f"Failed to insert from directory {directory_path}: {e}")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§

### 1. åµŒå…¥å‘é‡ä¼˜åŒ–

- **ç»´åº¦**ï¼š3072ç»´ï¼ˆä½¿ç”¨ `text-embedding-3-large`ï¼‰
- **å›é€€æœºåˆ¶**ï¼šOpenAIå¤±è´¥æ—¶ä½¿ç”¨å“ˆå¸ŒåµŒå…¥
- **æ‰¹å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡åµŒå…¥è®¡ç®—

### 2. ç¼“å­˜ä¼˜åŒ–

- **ç›¸ä¼¼åº¦é˜ˆå€¼**ï¼š0.90ï¼ˆå¯é…ç½®ï¼‰
- **ç¼“å­˜å¤§å°é™åˆ¶**ï¼š1000ä¸ªæ¡ç›®
- **FIFOç­–ç•¥**ï¼šè‡ªåŠ¨æ¸…ç†æ—§ç¼“å­˜

### 3. è¶…æ—¶æ§åˆ¶

- **LLMè°ƒç”¨è¶…æ—¶**ï¼š30åˆ†é’Ÿ
- **æŸ¥è¯¢è¶…æ—¶**ï¼š30ç§’ï¼ˆnaiveæ¨¡å¼ï¼‰ï¼Œ15ç§’ï¼ˆlocalæ¨¡å¼ï¼‰
- **é‡è¯•æœºåˆ¶**ï¼šæœ€å¤š5æ¬¡é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿

## ğŸ”§ é…ç½®å‚æ•°

### 1. æ ¸å¿ƒé…ç½®

```yaml
rag:
  lightrag:
    working_dir: "./lightrag_cache"  # å·¥ä½œç›®å½•
    enable_cache: true               # å¯ç”¨ç¼“å­˜
    cache_similarity_threshold: 0.90 # ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼
    openai:
      api_key: "your-api-key"       # OpenAI APIå¯†é’¥
```

### 2. æ¨¡å‹é…ç½®

```yaml
question_generator:
  local:
    model_name: "deepseek-r1:32b"   # æœ¬åœ°æ¨¡å‹åç§°
    base_url: "http://localhost:11434" # OllamaæœåŠ¡åœ°å€
    temperature: 0.7                 # æ¸©åº¦å‚æ•°
    max_tokens: 2048                 # æœ€å¤§ä»¤ç‰Œæ•°
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ–‡æ¡£é¢„å¤„ç†

- **æ ¼å¼ç»Ÿä¸€**ï¼šç¡®ä¿æ–‡æ¡£æ ¼å¼ä¸€è‡´
- **ç¼–ç å¤„ç†**ï¼šä½¿ç”¨UTF-8ç¼–ç 
- **å†…å®¹æ¸…ç†**ï¼šç§»é™¤æ— å…³çš„æ ¼å¼æ ‡è®°

### 2. æ‰¹é‡å¤„ç†

- **åˆ†æ‰¹å¤„ç†**ï¼šå¤§é‡æ–‡æ¡£åˆ†æ‰¹æ’å…¥
- **é”™è¯¯å¤„ç†**ï¼šå•ä¸ªæ–‡æ¡£å¤±è´¥ä¸å½±å“æ•´ä½“
- **è¿›åº¦ç›‘æ§**ï¼šä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦

### 3. æŸ¥è¯¢ä¼˜åŒ–

- **é—®é¢˜è§„èŒƒåŒ–**ï¼šç»Ÿä¸€é—®é¢˜æ ¼å¼
- **ç¼“å­˜åˆ©ç”¨**ï¼šå……åˆ†åˆ©ç”¨ç¼“å­˜æœºåˆ¶
- **è¶…æ—¶è®¾ç½®**ï¼šåˆç†è®¾ç½®è¶…æ—¶æ—¶é—´

### 4. å­˜å‚¨ç®¡ç†

- **ç›®å½•ç»“æ„**ï¼šä¿æŒå·¥ä½œç›®å½•æ•´æ´
- **å¤‡ä»½ç­–ç•¥**ï¼šå®šæœŸå¤‡ä»½çŸ¥è¯†å›¾è°±
- **æ¸…ç†æœºåˆ¶**ï¼šå®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. å…¼å®¹æ€§é—®é¢˜

- **tiktokenç¼–ç **ï¼šä½¿ç”¨ `cl100k_base` è€Œä¸æ˜¯ `o200k_base`
- **ç‰ˆæœ¬å…¼å®¹**ï¼šæ³¨æ„LightRAGç‰ˆæœ¬å…¼å®¹æ€§
- **ä¾èµ–ç®¡ç†**ï¼šç¡®ä¿æ‰€æœ‰ä¾èµ–æ­£ç¡®å®‰è£…

### 2. æ€§èƒ½è€ƒè™‘

- **å†…å­˜ä½¿ç”¨**ï¼šå¤§é‡æ–‡æ¡£å¯èƒ½æ¶ˆè€—å¤§é‡å†…å­˜
- **å¤„ç†æ—¶é—´**ï¼šçŸ¥è¯†å›¾è°±æ„å»ºéœ€è¦è¾ƒé•¿æ—¶é—´
- **å­˜å‚¨ç©ºé—´**ï¼šåµŒå…¥å‘é‡å ç”¨å¤§é‡å­˜å‚¨ç©ºé—´

### 3. é”™è¯¯å¤„ç†

- **ç½‘ç»œé—®é¢˜**ï¼šå¤„ç†APIè°ƒç”¨å¤±è´¥
- **æ¨¡å‹é—®é¢˜**ï¼šå¤„ç†æ¨¡å‹å“åº”å¼‚å¸¸
- **å­˜å‚¨é—®é¢˜**ï¼šå¤„ç†æ–‡ä»¶ç³»ç»Ÿé”™è¯¯

---

## ğŸ“š æ€»ç»“

LightRAG çŸ¥è¯†å›¾è°±ç”Ÿæˆæ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œæ¶‰åŠï¼š

1. **æ–‡æ¡£é¢„å¤„ç†**ï¼šæ–‡æœ¬åˆ†å—ã€æ ¼å¼æ¸…ç†
2. **å®ä½“æå–**ï¼šä½¿ç”¨LLMè¯†åˆ«å®ä½“
3. **å…³ç³»è¯†åˆ«**ï¼šåˆ†æå®ä½“é—´å…³ç³»
4. **å›¾è°±æ„å»º**ï¼šæ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±
5. **å‘é‡åŒ–å­˜å‚¨**ï¼šç”ŸæˆåµŒå…¥å‘é‡
6. **æŸ¥è¯¢ä¼˜åŒ–**ï¼šæ”¯æŒå¤šç§æŸ¥è¯¢æ¨¡å¼
7. **ç¼“å­˜æœºåˆ¶**ï¼šæé«˜æŸ¥è¯¢æ•ˆç‡

é€šè¿‡åˆç†çš„é…ç½®å’Œä¼˜åŒ–ï¼ŒLightRAG èƒ½å¤Ÿæ„å»ºé«˜è´¨é‡çš„çŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢å’Œé—®ç­”ç”Ÿæˆã€‚
