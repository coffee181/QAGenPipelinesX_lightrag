# QA Generation Pipelines Configuration - 本地模型版本

# OCR Configuration
ocr:
  provider: "paddle"
  paddle:
    lang: "ch"  # PaddleOCR 中文模型（支持中英混合）
    use_angle_cls: true
    dpi: 300


# Text Chunking Configuration
text_chunker:
  # 🚀 优化：使用 token 级切分，与 LightRAG 保持一致
  use_token_chunking: true  # 是否使用 token 级切分（推荐）
  tokenizer_model: "cl100k_base"  # Tokenizer 模型名称
  chunk_token_size: 1200  # 每个 chunk 的最大 token 数（与 LightRAG 默认值一致）
  chunk_overlap_token_size: 100  # Chunk 之间的重叠 token 数（与 LightRAG 默认值一致）
  
  # 字符级切分（向后兼容，当 use_token_chunking=false 时使用）
  max_chunk_size: 60000  # 最大块大小(字符数) - 仅在不使用 token 切分时生效
  overlap_size: 3000   # 重叠大小(字符数) - 仅在不使用 token 切分时生效
  chunk_on_sentences: true # 按句子分块（仅字符级切分时生效）
  
  # Chunk 持久化配置
  persist_chunks: false  # 已关闭本地chunk持久化
  chunk_store:
    type: "json"  # 存储类型：json, sqlite, postgres, redis
    path: "./lightrag_cache/chunks"  # 存储路径（使用已存在的lightrag_cache目录）

# Question Generation Configuration - 本地模型版本
question_generator:
  # 使用本地模型
  provider: "local"
  # local_scope（*_scope.json）导出已移除
  
  # Question quality optimization (NEW)
  enable_deduplication: true  # 启用问题去重
  dedup_similarity_threshold: 0.85  # 去重相似度阈值 (0-1)
  enable_quality_filter: true  # 启用问题质量过滤
  
  
  # 本地模型配置
  local:
    model_name: "deepseek-r1:32b"  # 本地模型名称
    base_url: "http://localhost:11434"  # Ollama服务地址（通过SSH隧道）
    max_tokens: 2048
    temperature: 0.7
    timeout: 30000  # 本地模型需要更长时间（500分钟，约8.3小时）
    questions_per_chunk: 10 # 减少问题数量以加快生成速度
    enable_kg_context: true
    max_context_entities: 5
    max_context_relations: 5
    max_context_snippets: 2
    context_snippet_chars: 200
    max_related_chunk_ids: 10
# RAG Configuration
rag:
  lightrag:
    working_dir: "F:/QAGenPipelinesX_lightrag/working/vectorized" # 你可以把它改成你自己向量化的路径"
    llm_model: "deepseek-chat"
    embed_model: "text-embedding-3-large"  # 仍然作为 OpenAI 回退模型
    enable_cache: true          # 是否开启检索结果缓存
    cache_similarity_threshold: 0.90  # 命中缓存所需的问题相似度阈值
    max_citations_per_answer: 5  # 答案中最多引用的 chunk 数量
    embedding:
      provider: "ollama"           # 本地嵌入模型提供方
      model: "bge-m3"              # Ollama 本地模型名称
      dim: 1024                    # bge-m3 的向量维度
      base_url: "http://localhost:11434"
      timeout: 1200                 # 单次调用超时（秒）
      max_retries: 3
      priority: ["ollama"]        # 仅本地模型
    llm:
      base_url: "http://localhost:11434"  # Ollama 推理服务地址
      model: "deepseek-r1:32b"            # LightRAG 使用的本地模型
      temperature: 0.7                    # 生成温度
      max_tokens: 2048                    # 最多生成 token 数
      timeout: 1800                       # 单次请求超时（秒）
      max_retries: 5                      # LLM 调用失败后的最大重试次数
    max_context_length: 4000
    query:
      top_k: 20                    # 每次检索知识图谱实体/关系的数量
      chunk_top_k: 10             # 每次保留的文本块数量
      max_entity_tokens: 10000     # 知识图谱实体上下文的 token 预算
      max_relation_tokens: 10000   # 知识图谱关系上下文的 token 预算
      max_total_tokens: 40000      # 整体上下文的 token 上限（实体+关系+chunk）
      cosine_threshold: 0.2        # 向量检索的最小余弦相似度
      related_chunk_number: 2      # 每个实体/关系允许附带的相关 chunk 数量
      history_turns: 0             # 查询时附带的历史对话轮数
      enable_rerank: true          # 是否启用 chunk 重排
      timeouts:
        mix: 1200   # mix 模式超时（秒）
        naive: 600  # naive 回退模式超时（秒）
        local: 600  # local 回退模式超时（秒）

# File Processing Configuration
file_processing:
  input_formats: [".pdf"]
  output_dir: "./output"
  temp_dir: "./temp"
  batch_size: 10
  
# Progress Management
progress:
  save_interval: 1  # 每处理1个文件保存一次进度，确保实时更新
  progress_file: "./progress.jsonl"
  enable_percentage_display: true  # 是否在控制台中展示整体完成百分比
  percentage_display_interval: 10  # 每多少条日志输出一次百分比
  
# Logging Configuration
logging:
  level: "INFO"
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
  file: "./logs/qa_gen.log"

# GSK Operations Prompts
prompts:
  system_prompt: |
    你是一名一线工业设备/数控系统的技术运维工程师，擅长把“现场现象 + 参数/工况 + 约束条件”组织成可落地的排障/优化问题。
    你深度理解 LightRAG 架构，能够从「已切分的 chunk 文本」与「知识图谱上下文」中提炼运维场景问答点。

    你生成的问题要像真实工程师在现场提问（参考：结论 + 操作建议 +（可选：推理依据、验证方案）的问法），但注意：你只输出问题，不输出答案。

    核心原则（必须遵守）：
    1. 严格基于提供的 <chunk>/<knowledge_graph> 提问；上下文缺什么就不要问，不允许推测补全
    2. 每个问题必须能在上下文内得到可验证回答：预期答案 ≤200 字，能直接给结论/参数建议/步骤/检查项
    3. 优先生成“诊断/优化型”运维问题：给出已知现象与关键参数，提出 1-2 个候选根因，并要求给出验证方案与不停机/最小干预的调整建议
    4. 避免泛化空问（“有哪些特点/为什么更好/适合哪些材料”）；问题必须具备可执行的检查点或可配置项
    5. 禁止引入上下文未出现的新型号/新参数/新名词

    文档标识：{document_id}（若chunk中没有具体实体信息，请在问题中明确指向该对象，而非泛指）

  human_prompt: |
    你是一名工业设备/数控系统技术运维工程师。请基于以下两个来源的上下文，生成 {questions_per_chunk} 个“运维诊断/优化型”的高质量问题。

    来源1：文档切分内容（chunk）
    <chunk>
    {text}
    </chunk>

    来源2：知识图谱上下文（来自LightRAG的实体、关系和相关chunk摘要）
    <knowledge_graph>
    {prompt_context}
    </knowledge_graph>

    文档标识：{document_id}（若chunk中没有具体实体信息，请在问题中明确指向该对象，而非泛指）

    严格禁止：
    1. 绝对不要编造、虚构、推测任何文档或知识图谱中没有的型号、参数或信息
    2. 不要使用类似 "RMD08"、"J1型号"、"J2型号" 等文档中不存在的名称
    3. 不要创造性地组合文档中的信息来生成新的型号或参数
    4. 如果文档中某个型号只有部分参数，不要为它编造其他参数的问题
    5. 禁止使用"这台/该/此/这个"等代词指代未在上下文出现的实体；必须点名具体对象
    6. 只输出问题，不要输出答案/Answer/解答，不要在问题行包含"答案"字样

    必须遵守：
    1. 只使用文档或知识图谱中明确出现的型号名称（例如：VMC850L、GSK 980TDi）
    2. 只使用文档或知识图谱中明确给出的参数和数值
    3. 问题中提到的每个实体和参数都必须能在<chunk>或<knowledge_graph>中找到
    4. 如果上下文缺少可用实体/数字，请少生成或返回空，不要编造
    5. 问题必须能在<chunk>或<knowledge_graph>中找到依据

    问题生成要求（参考你看到的案例风格，但必须以上下文为准）：
    1. 每个问题至少包含 2 个来自上下文的“可核对信息”：实体/型号/参数/数值/报警号/接口名/时间/阈值/范围之一
    2. 尽量加入“约束条件/不变量”来让问题更工程化，例如：
       - 在不改变 X（例如主轴转速/通道数/总线拓扑）的前提下，是否可通过调整 Y（参数/配置/张力/温度/阈值）缓解？
       - 不停机/最小干预条件下，如何验证候选根因？
    3. 问题结构建议（写成一个问题句即可，不要分段输出）：
       - 已知现象 + 当前关键参数/工况 + 怀疑根因A/B + 约束条件 + 你希望的输出格式
       - 输出格式应在问题里明确要求：请给出“结论 + 操作建议 +（可选：推理依据、验证方案）”
    4. 分布建议（当 {questions_per_chunk}=10 时）：
       - 4 个：诊断/排查型（含验证方案、最小干预）
       - 4 个：配置/参数/限制型（含生效条件/风险点）
       - 2 个：安全/保护/联锁/验收型（含阈值或明确条件）
    5. 保持可答性：如果 chunk 只给了功能描述而没有参数/步骤/条件，就不要生成“如何配置/如何排查”的问题

    输出格式（严格遵守，不要输出任何其他内容）：

    问题1: [具体问题内容]
    问题2: [具体问题内容]
    问题3: [具体问题内容]
    ...（依此类推）

    好问题示例（仅示意结构，不要照抄，必须以你看到的上下文为准）：
    - 在【某型号】运行中出现【某现象/报警号】，当前【参数A=..】【参数B=..】；这是否可能由【根因A】导致？在不改变【约束X】前提下，能否通过调整【参数/配置Y】缓解？请给出结论、操作建议，并给出快速验证方案。
    - 已知【接口/总线】配置为【值/模式】，现场出现【通讯/联锁/同步异常】；优先检查哪些【参数号/位号/从站ID】？在不停机条件下如何验证是【配置错误】还是【硬件链路】问题？请给出结论与排查步骤。

    避免的问题类型：
    - "XX有什么特点/优势？"（太宽泛且不利于短答）
    - "适合哪些材料/哪些工件？"（通常需要外部知识，除非上下文明确列出）
    - "为什么/原理是什么？"（除非上下文明确定义且可短答）
    - 不要输出标题、分类说明等格式内容
    - 不要输出"【复杂关联问题】"等分类标记
    - 不要输出markdown标题（如"#### XXX"）

    重要提示：
    - 直接输出问题，不要输出思考过程
    - 不要输出问题分类标题（如"复杂关联问题"）
    - 每个问题必须是完整的、可以直接用于提问的句子
    - 问题必须包含文档或知识图谱中的具体型号或参数
    - 预期答案应≤200字，直接给结论/数值/步骤，不要背景铺垫

  answer_system_prompt: |
    你是工业技术问答助手，回答必须完全基于检索到的内容，不足则明确说明"未找到依据"。
    输出 1-2 句、≤80 字，直接给结论/参数/步骤，保留必要单位与条件，禁止扩写背景或营销语。
    如有多个候选，仅列出最相关的 1-2 条并标注适用条件。

