"""LLMå®¢æˆ·ç«¯æ¨¡å— - å°è£…Ollama/DeepSeekäº¤äº’ä¸é—®é¢˜ç”Ÿæˆ"""

from __future__ import annotations

import hashlib
import json
import re
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, TYPE_CHECKING

import requests
from loguru import logger

if TYPE_CHECKING:
    from .rag_core import RAGCore


@dataclass
class DocumentChunk:
    """æ–‡æ¡£åˆ†å—"""
    document_id: str
    chunk_id: str
    content: str
    start_position: int = 0
    end_position: int = 0
    chunk_index: int = 0
    total_chunks: int = 0

    @property
    def length(self) -> int:
        return len(self.content)


@dataclass
class Question:
    """é—®é¢˜æ•°æ®ç»“æ„"""
    question_id: str
    content: str
    source_document: str
    source_chunk_id: str
    question_index: int
    created_at: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)
    source_chunk_content: Optional[str] = None
    related_entities: List[str] = field(default_factory=list)


@dataclass
class QuestionSet:
    """é—®é¢˜é›†"""
    document_id: str
    questions: List[Question]
    created_at: datetime

    @property
    def total_questions(self) -> int:
        return len(self.questions)

    def to_jsonl(self) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸ºJSONLæ ¼å¼"""
        return [
            {
                "question_id": q.question_id,
                "content": q.content,
                "source_document": q.source_document,
                "source_chunk_id": q.source_chunk_id,
                "question_index": q.question_index,
                "metadata": q.metadata,
                "created_at": q.created_at.isoformat() if q.created_at else None,
            }
            for q in self.questions
        ]


class LLMClient:
    """
    LLMå®¢æˆ·ç«¯ - å°è£…Ollamaäº¤äº’å’Œé—®é¢˜ç”Ÿæˆ
    """

    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "deepseek-r1:32b",
        max_tokens: int = 2048,
        temperature: float = 0.7,
        timeout: int = 30000,
        questions_per_chunk: int = 10,
        system_prompt: str = "",
        human_prompt: str = "",
        rag: Optional["RAGCore"] = None,
        kg_context_config: Optional[Dict[str, Any]] = None,
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            base_url: OllamaæœåŠ¡åœ°å€
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            questions_per_chunk: æ¯ä¸ªchunkç”Ÿæˆçš„é—®é¢˜æ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            human_prompt: ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
            rag: RAGæ ¸å¿ƒå®ä¾‹ï¼ˆç”¨äºè·å–çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼‰
            kg_context_config: çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡é…ç½®
        """
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.questions_per_chunk = questions_per_chunk
        self.system_prompt = system_prompt
        self.human_prompt = human_prompt
        self.rag = rag

        # çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡é…ç½®
        kg_cfg = kg_context_config or {}
        self.kg_enabled = kg_cfg.get("enabled", True)
        self.kg_max_entities = kg_cfg.get("max_entities", 5)
        self.kg_max_relations = kg_cfg.get("max_relations", 5)
        self.kg_max_snippets = kg_cfg.get("max_snippets", 2)
        self.kg_snippet_chars = kg_cfg.get("snippet_chars", 200)

        # æµ‹è¯•è¿æ¥
        if not self._test_connection():
            logger.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {self.base_url}")

        logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.model}")

    def _test_connection(self) -> bool:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            return response.status_code == 200
        except Exception:
            return False

    def generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        effective_system = system_prompt or self.system_prompt

        payload = {
            "model": self.model,
            "prompt": f"{effective_system}\n\n{prompt}" if effective_system else prompt,
            "stream": False,
            "options": {
                "temperature": self.temperature,
                "num_predict": self.max_tokens
            }
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )

            if response.status_code == 200:
                result = response.json()
                raw_text = result.get("response", "")
                return self._clean_think_tags(raw_text)
            else:
                raise RuntimeError(f"Ollama APIé”™è¯¯: {response.status_code} - {response.text}")

        except requests.exceptions.Timeout:
            raise RuntimeError(f"Ollama APIè¶…æ—¶ (è¶…è¿‡ {self.timeout} ç§’)")
        except Exception as e:
            raise RuntimeError(f"Ollama APIå¼‚å¸¸: {e}")

    def _clean_think_tags(self, text: str) -> str:
        """æ¸…ç†DeepSeek R1çš„<think>æ ‡ç­¾"""
        if not text:
            return ""
        cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        return cleaned.strip()

    def generate_questions_from_chunk(self, chunk: DocumentChunk) -> List[Question]:
        """
        ä»å•ä¸ªchunkç”Ÿæˆé—®é¢˜
        
        Args:
            chunk: æ–‡æ¡£åˆ†å—
            
        Returns:
            é—®é¢˜åˆ—è¡¨
        """
        logger.info(f"ğŸ” å¼€å§‹ä¸ºå—ç”Ÿæˆé—®é¢˜: {chunk.chunk_id}")
        logger.info(f"ğŸ“„ æ–‡æœ¬å—é•¿åº¦: {len(chunk.content)} å­—ç¬¦")

        # æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡
        context_package = self._build_context(chunk)
        prompt_context = context_package.get("prompt_context", "")

        # å‡†å¤‡æç¤ºè¯
        human_message = self.human_prompt.format(
            text=chunk.content.strip(),
            prompt_context=prompt_context,
            questions_per_chunk=self.questions_per_chunk,
            document_id=chunk.document_id,
        )

        logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(human_message)} å­—ç¬¦")
        logger.info(f"ğŸ¤– è°ƒç”¨æœ¬åœ°æ¨¡å‹: {self.model}")

        # è°ƒç”¨LLM
        response = self.generate(human_message, self.system_prompt)

        logger.info(f"âœ… æ”¶åˆ°å“åº”: {len(response)} å­—ç¬¦")

        # è§£æé—®é¢˜
        questions = self._parse_questions(response, chunk, context_package)

        logger.info(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(questions)} ä¸ªé—®é¢˜")
        return questions

    def generate_questions_from_chunks(self, chunks: List[DocumentChunk]) -> QuestionSet:
        """
        ä»å¤šä¸ªchunkç”Ÿæˆé—®é¢˜
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            
        Returns:
            é—®é¢˜é›†
        """
        if not chunks:
            raise ValueError("æ²¡æœ‰æä¾›ç”¨äºé—®é¢˜ç”Ÿæˆçš„å—")

        document_id = chunks[0].document_id
        logger.info(f"ä¸ºæ–‡æ¡£ {document_id} çš„ {len(chunks)} ä¸ªå—ç”Ÿæˆé—®é¢˜")

        all_questions = []
        for chunk in chunks:
            try:
                questions = self.generate_questions_from_chunk(chunk)
                all_questions.extend(questions)
            except Exception as e:
                logger.error(f"ä¸ºå— {chunk.chunk_id} ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
                continue

        question_set = QuestionSet(
            document_id=document_id,
            questions=all_questions,
            created_at=datetime.now()
        )

        logger.info(f"ä¸ºæ–‡æ¡£ {document_id} æ€»å…±ç”Ÿæˆäº† {len(all_questions)} ä¸ªé—®é¢˜")
        return question_set

    def _build_context(self, chunk: DocumentChunk) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡"""
        if not self.kg_enabled or not self.rag:
            return {"prompt_context": "", "related_entities": [], "related_chunk_ids": []}

        # è®¡ç®—chunk_id
        from .rag_core import compute_chunk_id
        chunk_id = compute_chunk_id(chunk.content)
        if not chunk_id:
            return {"prompt_context": "", "related_entities": [], "related_chunk_ids": []}

        try:
            context = self.rag.get_chunk_context(
                chunk_id,
                max_entities=self.kg_max_entities,
                max_relations=self.kg_max_relations,
                max_snippets=self.kg_max_snippets,
                snippet_chars=self.kg_snippet_chars,
            )
            return context
        except Exception as e:
            logger.debug(f"æ„å»ºçŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {"prompt_context": "", "related_entities": [], "related_chunk_ids": []}

    def _parse_questions(
        self,
        response: str,
        source_chunk: DocumentChunk,
        context_package: Dict[str, Any],
    ) -> List[Question]:
        """ä»LLMå“åº”ä¸­è§£æé—®é¢˜"""
        cleaned_response = self._clean_think_tags(response)
        questions: List[Question] = []

        base_entities = list(dict.fromkeys(context_package.get("related_entities", [])))
        base_chunk_ids = list(dict.fromkeys(context_package.get("related_chunk_ids", [])))
        
        # è®¡ç®—ä¸»chunk_id
        from .rag_core import compute_chunk_id
        primary_chunk_id = compute_chunk_id(source_chunk.content)
        
        if primary_chunk_id and primary_chunk_id not in base_chunk_ids:
            base_chunk_ids = [primary_chunk_id] + base_chunk_ids

        knowledge_used = bool(context_package.get("prompt_context"))

        # è§£æ"é—®é¢˜N:"æ ¼å¼
        question_pattern = r"é—®é¢˜(\d+)[:ï¼š]\s*(.+?)(?=\n\s*é—®é¢˜\d+[:ï¼š]|$)"
        matches = re.findall(question_pattern, cleaned_response, re.DOTALL)

        if matches:
            logger.info(f"âœ… æ‰¾åˆ° {len(matches)} ä¸ªé—®é¢˜")
            for match in matches:
                question_num = int(match[0])
                question_content = match[1].strip()
                question_content = re.sub(r"^é—®é¢˜[:ï¼š]\s*", "", question_content)
                question_content = re.sub(r"\n+", " ", question_content).strip()
                question_content = self._strip_answer(question_content)

                if self._is_valid_question(question_content):
                    question = self._build_question(
                        question_content, source_chunk, question_num,
                        base_entities, base_chunk_ids, primary_chunk_id, knowledge_used
                    )
                    questions.append(question)

        # å°è¯•å¤‡ç”¨è§£æ
        if not questions:
            logger.info("âš ï¸ å°è¯•å¤‡ç”¨è§£æ...")
            questions = self._fallback_parse(
                cleaned_response, source_chunk, context_package
            )

        return questions

    def _strip_answer(self, text: str) -> str:
        """ç§»é™¤ç­”æ¡ˆéƒ¨åˆ†"""
        if not text:
            return ""
        cleaned = re.sub(r"(ç­”æ¡ˆ[:ï¼š].*)", "", text, flags=re.IGNORECASE | re.DOTALL)
        cleaned = re.sub(r"(å›ç­”[:ï¼š].*)", "", cleaned, flags=re.IGNORECASE | re.DOTALL)
        cleaned = re.sub(r"(Answer[:ï¼š].*)", "", cleaned, flags=re.IGNORECASE | re.DOTALL)
        return cleaned.strip(" \t\r\nã€‚ï¼›;ï¼Œ,")

    def _is_valid_question(self, content: str) -> bool:
        """éªŒè¯é—®é¢˜æ˜¯å¦æœ‰æ•ˆ"""
        if not content or len(content) < 15:
            return False
        if not ("ï¼Ÿ" in content or "?" in content):
            return False
        if re.match(r"^#+\s", content):
            return False
        if re.match(r"^(å¤æ‚|ä¸­ç­‰|ç®€å•|å…³è”|æ·±åº¦|äº‹å®).*é—®é¢˜", content):
            return False
        if content.startswith("ã€"):
            return False
        if any(bad in content for bad in ["ç­”æ¡ˆ", "è§£ç­”", "Answer"]):
            return False
        return True

    def _build_question(
        self,
        content: str,
        source_chunk: DocumentChunk,
        question_index: int,
        base_entities: List[str],
        base_chunk_ids: List[str],
        primary_chunk_id: Optional[str],
        knowledge_used: bool,
    ) -> Question:
        """æ„å»ºQuestionå¯¹è±¡"""
        # ä»é—®é¢˜ä¸­æå–å®ä½“
        candidate_entities = self._extract_entities(content)
        combined_entities = list(dict.fromkeys(base_entities + candidate_entities))

        metadata: Dict[str, Any] = {"has_answer": False}
        if primary_chunk_id:
            metadata["lightrag_chunk_id"] = primary_chunk_id
        if combined_entities:
            metadata["related_entities"] = combined_entities
        if base_chunk_ids:
            metadata["related_chunk_ids"] = list(dict.fromkeys(base_chunk_ids))
        if knowledge_used:
            metadata["knowledge_context_used"] = True

        return Question(
            question_id=str(uuid.uuid4()),
            content=content,
            source_document=source_chunk.document_id,
            source_chunk_id=source_chunk.chunk_id,
            question_index=question_index,
            created_at=datetime.now(),
            metadata=metadata,
            source_chunk_content=source_chunk.content,
            related_entities=combined_entities,
        )

    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        if not text:
            return []

        candidates: List[str] = []
        patterns = [
            r"[A-Z]{2,}\d+[A-Z]*",
            r"[A-Z]+\d+[A-Z0-9]*",
            r"[A-Z][A-Za-z0-9\-]{2,}",
        ]

        matched_tokens = set()
        for pattern in patterns:
            matched_tokens.update(re.findall(pattern, text))

        for token in re.findall(r"\b[^\s]+\b", text):
            normalized = token.strip(".,;:!?ï¼Œã€‚ï¼›ï¼šï¼ˆï¼‰()[]{}""\"'")
            if normalized in matched_tokens and normalized not in candidates:
                candidates.append(normalized)

        return candidates

    def _fallback_parse(
        self,
        response: str,
        source_chunk: DocumentChunk,
        context_package: Dict[str, Any],
    ) -> List[Question]:
        """å¤‡ç”¨é—®é¢˜è§£æ"""
        questions = []
        
        from .rag_core import compute_chunk_id
        
        base_entities = list(dict.fromkeys(context_package.get("related_entities", [])))
        base_chunk_ids = list(dict.fromkeys(context_package.get("related_chunk_ids", [])))
        primary_chunk_id = compute_chunk_id(source_chunk.content)
        
        if primary_chunk_id and primary_chunk_id not in base_chunk_ids:
            base_chunk_ids = [primary_chunk_id] + base_chunk_ids
        knowledge_used = bool(context_package.get("prompt_context"))

        lines = response.split('\n')
        question_index = 1

        for line in lines:
            line = line.strip()
            if not line:
                continue
            if line.startswith('#') or line.startswith('ã€'):
                continue
            if re.match(r'^(å¤æ‚|ä¸­ç­‰|ç®€å•|å…³è”|æ·±åº¦|äº‹å®).*é—®é¢˜', line):
                continue

            if '?' in line or 'ï¼Ÿ' in line or line.startswith(('å¦‚ä½•', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€æ ·', 'å“ªäº›', 'æ˜¯å¦')):
                cleaned = re.sub(r'^[\d\.\-\*\s]+', '', line)
                cleaned = re.sub(r'^é—®é¢˜\d+[:\ï¼š]\s*', '', cleaned)
                cleaned = self._strip_answer(cleaned)

                if self._is_valid_question(cleaned):
                    question = self._build_question(
                        cleaned, source_chunk, question_index,
                        base_entities, base_chunk_ids, primary_chunk_id, knowledge_used
                    )
                    questions.append(question)
                    question_index += 1

                    if len(questions) >= self.questions_per_chunk:
                        break

        return questions


class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(
        self,
        use_token_chunking: bool = True,
        tokenizer_model: str = "cl100k_base",
        chunk_token_size: int = 1200,
        chunk_overlap_token_size: int = 100,
        max_chunk_size: int = 60000,
        overlap_size: int = 3000,
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            use_token_chunking: æ˜¯å¦ä½¿ç”¨tokençº§åˆ†å—
            tokenizer_model: tokenizeræ¨¡å‹
            chunk_token_size: æ¯ä¸ªchunkçš„tokenæ•°
            chunk_overlap_token_size: é‡å tokenæ•°
            max_chunk_size: æœ€å¤§å­—ç¬¦æ•°ï¼ˆå­—ç¬¦çº§åˆ†å—ï¼‰
            overlap_size: é‡å å­—ç¬¦æ•°ï¼ˆå­—ç¬¦çº§åˆ†å—ï¼‰
        """
        self.use_token_chunking = use_token_chunking
        self.tokenizer_model = tokenizer_model
        self.chunk_token_size = chunk_token_size
        self.chunk_overlap_token_size = chunk_overlap_token_size
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size

        self.tokenizer = None
        if use_token_chunking:
            try:
                import tiktoken
                self.tokenizer = tiktoken.get_encoding(tokenizer_model)
                logger.info(f"Tokenåˆ†å—å™¨åˆå§‹åŒ–å®Œæˆ: {tokenizer_model}")
            except Exception as e:
                logger.warning(f"Tokenizeråˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å­—ç¬¦çº§åˆ†å—")
                self.use_token_chunking = False

    def chunk_text(self, text: str, document_id: str) -> List[DocumentChunk]:
        """
        å°†æ–‡æœ¬åˆ†å—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            document_id: æ–‡æ¡£ID
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        if not text.strip():
            return []

        if self.use_token_chunking and self.tokenizer:
            return self._chunk_by_tokens(text, document_id)
        else:
            return self._chunk_by_chars(text, document_id)

    def _chunk_by_tokens(self, text: str, document_id: str) -> List[DocumentChunk]:
        """Tokençº§åˆ†å—"""
        try:
            from lightrag.operate import chunking_by_token_size
            
            chunk_dicts = chunking_by_token_size(
                tokenizer=self.tokenizer,
                content=text,
                split_by_character=None,
                split_by_character_only=False,
                overlap_token_size=self.chunk_overlap_token_size,
                max_token_size=self.chunk_token_size
            )
        except ImportError:
            logger.warning("LightRAG chunkingä¸å¯ç”¨ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ†å—")
            return self._chunk_by_chars(text, document_id)

        chunks = []
        current_pos = 0

        for idx, chunk_dict in enumerate(chunk_dicts):
            content = chunk_dict.get("content", "").strip()
            if not content:
                continue

            from .rag_core import compute_chunk_id
            chunk_id = compute_chunk_id(content) or f"{document_id}_chunk_{idx}"

            start_pos = text.find(content, current_pos)
            end_pos = start_pos + len(content)
            current_pos = start_pos

            chunk = DocumentChunk(
                document_id=document_id,
                chunk_id=chunk_id,
                content=content,
                start_position=start_pos,
                end_position=end_pos,
                chunk_index=chunk_dict.get("chunk_order_index", idx),
                total_chunks=len(chunk_dicts)
            )
            chunks.append(chunk)

        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        logger.info(f"Tokenåˆ†å—å®Œæˆ: {len(chunks)} ä¸ªchunks")
        return chunks

    def _chunk_by_chars(self, text: str, document_id: str) -> List[DocumentChunk]:
        """å­—ç¬¦çº§åˆ†å—"""
        chunks = []
        start = 0

        while start < len(text):
            end = min(start + self.max_chunk_size, len(text))
            content = text[start:end]

            from .rag_core import compute_chunk_id
            chunk_id = compute_chunk_id(content) or f"{document_id}_chunk_{len(chunks)}"

            chunk = DocumentChunk(
                document_id=document_id,
                chunk_id=chunk_id,
                content=content,
                start_position=start,
                end_position=end,
                chunk_index=len(chunks),
                total_chunks=0
            )
            chunks.append(chunk)

            start = end - self.overlap_size if end < len(text) else end

        for chunk in chunks:
            chunk.total_chunks = len(chunks)

        logger.info(f"å­—ç¬¦åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªchunks")
        return chunks

